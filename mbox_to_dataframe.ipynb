{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98258d6f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fed628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mailbox\n",
    "import pandas as pd\n",
    "import email\n",
    "import os\n",
    "from email.header import decode_header\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import html\n",
    "import sys\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c725da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57cc511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.models import EmailAddress, MailingList, Organisation, Position, Entity, Attachment, ReceiverEmail, SenderEmail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d6cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attempt #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c430afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_str(s):\n",
    "    \"\"\"Decode encoded email header strings\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    decoded_parts = decode_header(s)\n",
    "    return ''.join([\n",
    "        part.decode(encoding or 'utf-8', errors='replace') if isinstance(part, bytes) else part\n",
    "        for part, encoding in decoded_parts\n",
    "    ])\n",
    "\n",
    "def mbox_to_dataframe(mbox_path):\n",
    "    \"\"\"Convert an mbox file to a pandas DataFrame\"\"\"\n",
    "    # Open the mbox file\n",
    "    mbox = mailbox.mbox(mbox_path)\n",
    "\n",
    "    # Extract folder name from path\n",
    "    folder_name = os.path.basename(mbox_path).replace('.mbox', '')\n",
    "\n",
    "    # Create a list to store email data\n",
    "    emails = []\n",
    "\n",
    "    # Process each message\n",
    "    for i, message in enumerate(mbox):\n",
    "        # Extract basic headers\n",
    "        subject = decode_str(message['subject'])\n",
    "        from_addr = decode_str(message['from'])\n",
    "        to_addr = decode_str(message['to'])\n",
    "        date_str = message['date']\n",
    "\n",
    "        # Parse date\n",
    "        try:\n",
    "            date = email.utils.parsedate_to_datetime(date_str)\n",
    "        except:\n",
    "            date = None\n",
    "\n",
    "        # Get message body\n",
    "        body = \"\"\n",
    "        if message.is_multipart():\n",
    "            for part in message.walk():\n",
    "                content_type = part.get_content_type()\n",
    "                content_disposition = str(part.get(\"Content-Disposition\"))\n",
    "\n",
    "                # Skip attachments\n",
    "                if \"attachment\" in content_disposition:\n",
    "                    continue\n",
    "\n",
    "                # Get text content\n",
    "                if content_type == \"text/plain\":\n",
    "                    try:\n",
    "                        body_part = part.get_payload(decode=True)\n",
    "                        charset = part.get_content_charset() or 'utf-8'\n",
    "                        body += body_part.decode(charset, errors='replace')\n",
    "                    except:\n",
    "                        body += \"[Error decoding message body]\"\n",
    "        else:\n",
    "            # Not multipart - get payload directly\n",
    "            try:\n",
    "                body = message.get_payload(decode=True).decode(message.get_content_charset() or 'utf-8', errors='replace')\n",
    "            except:\n",
    "                body = \"[Error decoding message body]\"\n",
    "\n",
    "        # Add to emails list\n",
    "        emails.append({\n",
    "            'message_id': message['message-id'],\n",
    "            'subject': subject,\n",
    "            'from': from_addr,\n",
    "            'to': to_addr,\n",
    "            'date': date,\n",
    "            'folder': folder_name,\n",
    "            'body': body,\n",
    "            'has_attachments': \"attachment\" in str(message).lower()\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    return pd.DataFrame(emails)\n",
    "\n",
    "# Process all mbox files in a directory\n",
    "def process_all_mbox_files(directory):\n",
    "    all_emails = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.mbox'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f\"Processing {filename}...\")\n",
    "            df = mbox_to_dataframe(file_path)\n",
    "            all_emails = pd.concat([all_emails, df], ignore_index=True)\n",
    "\n",
    "    return all_emails\n",
    "\n",
    "# Example usage:\n",
    "# directory = \"/path/to/your/mbox/files\"\n",
    "# emails_df = process_all_mbox_files(directory)\n",
    "\n",
    "# Save to various formats:\n",
    "# emails_df.to_csv('emails.csv', index=False)\n",
    "# emails_df.to_pickle('emails.pkl')  # Pandas format\n",
    "# emails_df.to_parquet('emails.parquet')  # Parquet format\n",
    "\n",
    "# To save to SQLite database:\n",
    "# import sqlite3\n",
    "# conn = sqlite3.connect('emails.db')\n",
    "# emails_df.to_sql('emails', conn, index=False, if_exists='replace')\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b160d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AG.mbox...\n",
      "Processing Archive.mbox...\n",
      "Processing Archives calssifiees.mbox...\n",
      "Processing Ateliers.mbox...\n",
      "Processing Boîte de réception.mbox...\n",
      "Processing Brouillons.mbox...\n",
      "Processing Conflit.mbox...\n",
      "Processing Courrier indésirable.mbox...\n",
      "Processing Formation à distance.mbox...\n",
      "Processing Gazette.mbox...\n",
      "Processing gestioncrise.mbox...\n",
      "Processing Idees.mbox...\n",
      "Processing Instances.mbox...\n",
      "Processing Plaidoyer.mbox...\n",
      "Processing RH.mbox...\n",
      "Processing Éléments envoyés.mbox...\n",
      "Processing Éléments supprimés.mbox...\n"
     ]
    }
   ],
   "source": [
    "emails_df = process_all_mbox_files(\"data/processed/mailbox_cecile/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32c972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>date</th>\n",
       "      <th>folder</th>\n",
       "      <th>body</th>\n",
       "      <th>has_attachments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;0e8d36fce37f4240b8b7f0965b794923@archivistes....</td>\n",
       "      <td>TR: Rappel : Assemblée générale de l'AAF le 29...</td>\n",
       "      <td>AAF - Anne Clerc, déléguée générale&lt;delegation...</td>\n",
       "      <td>AAF - Céline Guyon &lt;celine.guyon@archivistes.org&gt;</td>\n",
       "      <td>2020-06-10 16:22:01+02:00</td>\n",
       "      <td>AG</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          message_id  \\\n",
       "0  <0e8d36fce37f4240b8b7f0965b794923@archivistes....   \n",
       "\n",
       "                                             subject  \\\n",
       "0  TR: Rappel : Assemblée générale de l'AAF le 29...   \n",
       "\n",
       "                                                from  \\\n",
       "0  AAF - Anne Clerc, déléguée générale<delegation...   \n",
       "\n",
       "                                                  to  \\\n",
       "0  AAF - Céline Guyon <celine.guyon@archivistes.org>   \n",
       "\n",
       "                        date folder body  has_attachments  \n",
       "0  2020-06-10 16:22:01+02:00     AG                  True  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_df.head(1)\n",
    "\n",
    "# </v:shape><![endif]--><![if !vml]><img width=143 height=201 style='width:1.4895in;height:2.0937in' src=\"cid:image007.jpg@01D63F43.70FD3750\" align=left hspace=12 v:shapes=\"Image_x0020_1\"><![endif]><b><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:#9DC107;mso-fareast-language:FR'><o:p></o:p></span></b></p><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:#9DC107;mso-fareast-language:FR'>Anne Clerc<o:p></o:p></span></b></p><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:#9DC107;mso-fareast-language:FR'>D�l�gu�e G�n�rale<o:p></o:p></span></b></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR'>Association des archivistes fran�ais<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR'>8 rue Jean-Marie J�go<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR'>75013 PARIS<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR'>T�l. 01 46 06 40 12<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR'>Portable 06 79 53 47 40<o:p></o:p></span></p><p class=MsoNormal><span style='mso-fareast-language:FR'><a href=\"mailto:vieassociative@archivistes.org\"><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:blue'>delegation_generale@archivistes.org</span></a></span><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR'><o:p></o:p></span></p><p class=MsoNormal><span style='mso-fareast-language:FR'><a href=\"https://www.archivistes.org/\"><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:blue'>https://www.archivistes.org</span></a></span><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR'><o:p></o:p></span></p><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:#9DC107;mso-fareast-language:FR'><o:p>&nbsp;</o:p></span></b></p><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal><o:p>&nbsp;</o:p></p><div><div style='border:none;border-top:solid #E1E1E1 1.0pt;padding:3.0pt 0cm 0cm 0cm'><p class=MsoNormal><b><span style='mso-fareast-language:FR'>De&nbsp;:</span></b><span style='mso-fareast-language:FR'> Daniel BUCHOUX &lt;d.buchoux@aca.nexia.fr&gt; <br><b>Envoy�&nbsp;:</b> mercredi 10 juin 2020 16:17<br><b>�&nbsp;:</b> AAF - Anne Clerc, d�l�gu�e g�n�rale &lt;delegation_generale@archivistes.org&gt;; Dolly RAZAFINANJA &lt;d.razafinanja@aca.nexia.fr&gt;; Simon DELEDICQ &lt;s.deledicq@aca.nexia.fr&gt;<br><b>Objet&nbsp;:</b> RE: Rappel : Assembl�e g�n�rale de l'AAF le 29 juin 2020 � partir de 17h en visioconf�rence<o:p></o:p></span></p></div></div><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:Effra;color:#575756'>Bonjour Anne,<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:Effra;color:#575756'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:Effra;color:#575756'>J&#8217;esp�re que vous allez bien.<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:Effra;color:#575756'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:Effra;color:#575756'>Je serai pr�sent.<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:Effra;color:#575756'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:Effra;color:#575756'>Bonne journ�e.<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:10.0pt;font-family:Effra;color:#575756'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:Effra;color:#00B2A9;mso-fareast-language:FR'>Daniel Buchoux </span></b><span style='font-size:9.0pt;font-family:Effra;color:#575756;mso-fareast-language:FR'><br><br>Aca Nexia </span><b><span style='font-size:9.0pt;font-family:Effra;color:#00B2A9;mso-fareast-language:FR'>|</span></b><span style='font-size:9.0pt;font-family:Effra;color:#575756;mso-fareast-language:FR'> 31, rue Henri Rochefort, 75017 Paris, France <br></span><b><span style='font-size:9.0pt;font-family:Effra;color:#00B2A9;mso-fareast-language:FR'>T</span></b><span style='font-size:9.0pt;font-family:Effra;color:#575756;mso-fareast-language:FR'> +33 (0)1 47 66 77 88 </span><b><span style='font-size:9.0pt;font-family:Effra;color:#00B2A9;mso-fareast-language:FR'>| M</span></b><span style='font-size:9.0pt;font-family:Effra;color:#575756;mso-fareast-language:FR'> +33 (0)6 07 76 25 49 <br></span><b><span style='font-size:9.0pt;font-family:Effra;color:#00B2A9;mso-fareast-language:FR'>E</span></b><span style='font-size:9.0pt;font-family:Effra;color:#575756;mso-fareast-language:FR'> <a href=\"mailto:d.buchoux@aca.nexia.fr\"><span style='color:#575756'>d.buchoux@aca.nexia.fr</span></a> <br><a href=\"http://www.aca.nexia.fr\"><span style='color:#575756'>aca.nexia.fr</span></a> <br><br><img border=0 width=100 height=39 style='width:1.0416in;height:.4062in' id=\"Image_x0020_9\" src=\"cid:image008.png@01D63F43.70FD3750\" alt=\"Aca, Independant member of Nexia International\">&nbsp; </span><span style='font-size:10.0pt;font-family:Effra;color:#575756'><o:p></o:p></span></p><div><div style='border:none;border-top:solid #E1E1E1 1.0pt;padding:3.0pt 0cm 0cm 0cm'><p class=MsoNormal><b><span style='mso-fareast-language:FR'>De&nbsp;:</span></b><span style='mso-fareast-language:FR'> AAF - Anne Clerc, d�l�gu�e g�n�rale &lt;delegation_generale@archivistes.org&gt; <br><b>Envoy�&nbsp;:</b> mercredi 10 juin 2020 15:53<br><b>�&nbsp;:</b> Dolly RAZAFINANJA &lt;d.razafinanja@aca.nexia.fr&gt;; Daniel BUCHOUX &lt;d.buchoux@aca.nexia.fr&gt;; Simon DELEDICQ &lt;s.deledicq@aca.nexia.fr&gt;<br><b>Objet&nbsp;:</b> TR: Rappel : Assembl�e g�n�rale de l'AAF le 29 juin 2020 � partir de 17h en visioconf�rence<o:p></o:p></span></p></div></div><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal>Bonjour, <o:p></o:p></p><p class=MsoNormal>&nbsp;<o:p></o:p></p><p class=MsoNormal>Pour information, je vous transmets l&#8217;invitation pour notre AG. En comptant sur votre pr�sence.<o:p></o:p></p><p class=MsoNormal><br>Cordialement<o:p></o:p></p><p class=MsoNormal>&nbsp;<o:p></o:p></p><p class=MsoNormal>&nbsp;<o:p></o:p></p><p class=MsoNormal><!--[if gte vml 1]><v:shape id=\"Image_x0020_2\" o:spid=\"_x0000_s1026\" type=\"#_x0000_t75\" style='position:absolute;margin-left:0;margin-top:0;width:107.25pt;height:150.75pt;z-index:251658240;visibility:visible;mso-wrap-style:square;mso-width-percent:0;mso-height-percent:0;mso-wrap-distance-left:4.5pt;mso-wrap-distance-top:0;mso-wrap-distance-right:4.5pt;mso-wrap-distance-bottom:0;mso-position-horizontal:left;mso-position-horizontal-relative:text;mso-position-vertical:absolute;mso-position-vertical-relative:line;mso-width-percent:0;mso-height-percent:0;mso-width-relative:page;mso-height-relative:page' o:allowoverlap=\"f\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b06cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "folder\n",
       "Boîte de réception      318\n",
       "Éléments supprimés       12\n",
       "Courrier indésirable      6\n",
       "Archives calssifiees      4\n",
       "Archive                   1\n",
       "Éléments envoyés          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_df[emails_df[\"body\"] != \"\"][\"folder\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521eb635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: body, dtype: object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bonjour et grand Merci Lucile\n",
    "emails_df[\"body\"][emails_df[\"body\"].str.find(\"L'ordre du jour de l'assem\") != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da165d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         \n",
       "1         \n",
       "2         \n",
       "3         \n",
       "4         \n",
       "        ..\n",
       "19130     \n",
       "19131     \n",
       "19132     \n",
       "19133     \n",
       "19135     \n",
       "Name: body, Length: 18794, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_df[\"body\"][emails_df[\"body\"] == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e203d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_df[\"body\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd8bf0",
   "metadata": {},
   "source": [
    "## Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f398b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def decode_str(s):\n",
    "    \"\"\"Decode encoded email header strings\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        decoded_parts = decode_header(s)\n",
    "        return ''.join([\n",
    "            part.decode(encoding or 'utf-8', errors='replace') if isinstance(part, bytes) else part\n",
    "            for part, encoding in decoded_parts\n",
    "        ])\n",
    "    except:\n",
    "        return str(s)\n",
    "\n",
    "def extract_text_from_html(html_content):\n",
    "    \"\"\"Extract readable text from HTML content\"\"\"\n",
    "    if not html_content:\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        # Get text\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        return text\n",
    "    except:\n",
    "        return html_content  # Return original if parsing fails\n",
    "\n",
    "def get_email_body(message):\n",
    "    \"\"\"Extract body text from email message, handling HTML correctly\"\"\"\n",
    "    body_text = \"\"\n",
    "    body_html = \"\"\n",
    "\n",
    "    if message.is_multipart():\n",
    "        for part in message.walk():\n",
    "            content_type = part.get_content_type()\n",
    "            content_disposition = str(part.get(\"Content-Disposition\") or \"\")\n",
    "\n",
    "            # Skip attachments\n",
    "            if \"attachment\" in content_disposition:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                payload = part.get_payload(decode=True)\n",
    "                if payload is None:\n",
    "                    continue\n",
    "\n",
    "                charset = part.get_content_charset() or 'utf-8'\n",
    "                decoded_payload = payload.decode(charset, errors='replace')\n",
    "\n",
    "                if content_type == \"text/plain\":\n",
    "                    body_text += decoded_payload\n",
    "                elif content_type == \"text/html\":\n",
    "                    body_html += decoded_payload\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        # Not multipart - get payload directly\n",
    "        try:\n",
    "            content_type = message.get_content_type()\n",
    "            payload = message.get_payload(decode=True)\n",
    "            if payload:\n",
    "                charset = message.get_content_charset() or 'utf-8'\n",
    "                decoded_payload = payload.decode(charset, errors='replace')\n",
    "\n",
    "                if content_type == \"text/plain\":\n",
    "                    body_text = decoded_payload\n",
    "                elif content_type == \"text/html\":\n",
    "                    body_html = decoded_payload\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Prefer HTML content but fall back to plain text\n",
    "    if body_html:\n",
    "        return {\n",
    "            \"html\": body_html,\n",
    "            \"text\": extract_text_from_html(body_html),\n",
    "            \"has_html\": True\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"html\": \"\",\n",
    "            \"text\": body_text,\n",
    "            \"has_html\": False\n",
    "        }\n",
    "\n",
    "def extract_attachments_info(message):\n",
    "    \"\"\"Extract information about attachments\"\"\"\n",
    "    attachments = []\n",
    "\n",
    "    if message.is_multipart():\n",
    "        for part in message.walk():\n",
    "            content_disposition = str(part.get(\"Content-Disposition\") or \"\")\n",
    "\n",
    "            if \"attachment\" in content_disposition:\n",
    "                filename = part.get_filename()\n",
    "                if filename:\n",
    "                    try:\n",
    "                        filename = decode_str(filename)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                content_type = part.get_content_type()\n",
    "                attachments.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"content_type\": content_type,\n",
    "                    \"size\": len(part.get_payload(decode=True) or b'')\n",
    "                })\n",
    "\n",
    "    return attachments\n",
    "\n",
    "def extract_recipients(message):\n",
    "    \"\"\"Extract all recipients (To, CC, BCC)\"\"\"\n",
    "    to = decode_str(message.get('to') or \"\")\n",
    "    cc = decode_str(message.get('cc') or \"\")\n",
    "    bcc = decode_str(message.get('bcc') or \"\")\n",
    "\n",
    "    return {\n",
    "        \"to\": to,\n",
    "        \"cc\": cc,\n",
    "        \"bcc\": bcc\n",
    "    }\n",
    "\n",
    "def extract_message_data(message, folder_name):\n",
    "    \"\"\"Extract comprehensive email data\"\"\"\n",
    "    # Extract basic headers\n",
    "    subject = decode_str(message.get('subject') or \"\")\n",
    "    from_addr = decode_str(message.get('from') or \"\")\n",
    "    date_str = message.get('date')\n",
    "    message_id = decode_str(message.get('message-id') or \"\")\n",
    "\n",
    "    # Parse date\n",
    "    try:\n",
    "        date = email.utils.parsedate_to_datetime(date_str)\n",
    "    except:\n",
    "        date = None\n",
    "\n",
    "    # Get recipients\n",
    "    recipients = extract_recipients(message)\n",
    "\n",
    "    # Get body content\n",
    "    body_content = get_email_body(message)\n",
    "\n",
    "    # Get attachment info\n",
    "    attachments = extract_attachments_info(message)\n",
    "\n",
    "    # Thread information\n",
    "    references = decode_str(message.get('references') or \"\")\n",
    "    in_reply_to = decode_str(message.get('in-reply-to') or \"\")\n",
    "\n",
    "    return {\n",
    "        'message_id': message_id,\n",
    "        'subject': subject,\n",
    "        'from': from_addr,\n",
    "        'to': recipients[\"to\"],\n",
    "        'cc': recipients[\"cc\"],\n",
    "        'bcc': recipients[\"bcc\"],\n",
    "        'date': date,\n",
    "        'folder': folder_name,\n",
    "        'body_text': body_content[\"text\"],\n",
    "        'body_html': body_content[\"html\"],\n",
    "        'has_html': body_content[\"has_html\"],\n",
    "        'attachments': attachments,\n",
    "        'attachment_count': len(attachments),\n",
    "        'references': references,\n",
    "        'in_reply_to': in_reply_to\n",
    "    }\n",
    "\n",
    "def process_mbox_to_sqlite(mbox_path, conn, batch_size=100):\n",
    "    \"\"\"Process mbox file directly to SQLite in batches with progress bar\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    mbox = mailbox.mbox(mbox_path)\n",
    "    folder_name = os.path.basename(mbox_path).replace('.mbox', '')\n",
    "\n",
    "    # Count messages for progress bar\n",
    "    total_messages = len(mbox)\n",
    "    print(f\"Processing {folder_name} ({total_messages} messages)\")\n",
    "\n",
    "    # Process in batches\n",
    "    batch = []\n",
    "    for i, message in enumerate(tqdm(mbox, total=total_messages, desc=folder_name)):\n",
    "        email_data = extract_message_data(message, folder_name)\n",
    "\n",
    "        # Convert attachment data to string\n",
    "        if email_data['attachments']:\n",
    "            email_data['attachments_json'] = str(email_data['attachments'])\n",
    "        else:\n",
    "            email_data['attachments_json'] = \"\"\n",
    "\n",
    "        del email_data['attachments']  # Remove original list\n",
    "\n",
    "        batch.append(email_data)\n",
    "\n",
    "        # Process batch\n",
    "        if len(batch) >= batch_size or i == total_messages - 1:\n",
    "            if batch:\n",
    "                # Convert to DataFrame for easy SQL insertion\n",
    "                df = pd.DataFrame(batch)\n",
    "\n",
    "                # Write to database\n",
    "                df.to_sql('emails', conn, if_exists='append', index=False)\n",
    "\n",
    "                # Clear batch\n",
    "                batch = []\n",
    "\n",
    "                # Commit to save progress\n",
    "                conn.commit()\n",
    "\n",
    "def setup_database(db_path):\n",
    "    \"\"\"Set up the SQLite database schema with proper types and indexes\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Drop table if exists\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS emails\")\n",
    "\n",
    "    # Create table with proper columns and types\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE emails (\n",
    "        message_id TEXT,\n",
    "        subject TEXT,\n",
    "        \"from\" TEXT,\n",
    "        \"to\" TEXT,\n",
    "        cc TEXT,\n",
    "        bcc TEXT,\n",
    "        date TIMESTAMP,\n",
    "        folder TEXT,\n",
    "        body_text TEXT,\n",
    "        body_html TEXT,\n",
    "        has_html BOOLEAN,\n",
    "        attachments_json TEXT,\n",
    "        attachment_count INTEGER,\n",
    "        references TEXT,\n",
    "        in_reply_to TEXT\n",
    "    )\n",
    "    ''')\n",
    "\n",
    "    # Create indexes for common queries\n",
    "    cursor.execute('CREATE INDEX idx_date ON emails(date)')\n",
    "    cursor.execute('CREATE INDEX idx_folder ON emails(folder)')\n",
    "    cursor.execute('CREATE INDEX idx_from ON emails(\"from\")')\n",
    "    cursor.execute('CREATE INDEX idx_to ON emails(\"to\")')\n",
    "    cursor.execute('CREATE INDEX idx_subject ON emails(subject)')\n",
    "\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def process_all_mbox_files(directory, db_path='emails.db'):\n",
    "    \"\"\"Process all mbox files in directory to a SQLite database\"\"\"\n",
    "    # Setup database\n",
    "    conn = setup_database(db_path)\n",
    "\n",
    "    # Get list of mbox files\n",
    "    mbox_files = [f for f in os.listdir(directory) if f.endswith('.mbox')]\n",
    "    print(f\"Found {len(mbox_files)} mbox files\")\n",
    "\n",
    "    # Process each file\n",
    "    for filename in mbox_files:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        process_mbox_to_sqlite(file_path, conn)\n",
    "\n",
    "    # Create full-text search index for body\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"CREATE VIRTUAL TABLE IF NOT EXISTS email_fts USING fts5(body_text, content='emails', content_rowid='rowid')\")\n",
    "        cursor.execute(\"INSERT INTO email_fts(rowid, body_text) SELECT rowid, body_text FROM emails\")\n",
    "    except:\n",
    "        print(\"Warning: Full-text search index creation failed. SQLite may not have FTS5 support.\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Processing complete. Database saved to {db_path}\")\n",
    "\n",
    "# Example usage:\n",
    "directory = \"data/processed/mailbox_cecile/\"\n",
    "# process_all_mbox_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6206a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 mbox files\n",
      "Processing AG (6 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AG: 100%|██████████| 6/6 [00:00<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Archive (10 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Archive: 100%|██████████| 10/10 [00:00<00:00, 22.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Archives calssifiees (423 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Archives calssifiees: 100%|██████████| 423/423 [00:20<00:00, 21.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Ateliers (28 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ateliers: 100%|██████████| 28/28 [00:00<00:00, 66.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Boîte de réception (12499 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Boîte de réception: 100%|██████████| 12499/12499 [02:42<00:00, 77.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Brouillons (41 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Brouillons: 100%|██████████| 41/41 [00:00<00:00, 146.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Conflit (6 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conflit: 100%|██████████| 6/6 [00:00<00:00, 52.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Courrier indésirable (45 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Courrier indésirable: 100%|██████████| 45/45 [00:00<00:00, 73.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Formation à distance (2 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formation à distance: 100%|██████████| 2/2 [00:00<00:00, 48.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gazette (10 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gazette: 100%|██████████| 10/10 [00:00<00:00, 94.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing gestioncrise (74 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gestioncrise: 100%|██████████| 74/74 [00:00<00:00, 89.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Idees (18 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Idees: 100%|██████████| 18/18 [00:00<00:00, 120.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Instances (60 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instances: 100%|██████████| 60/60 [00:00<00:00, 85.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Plaidoyer (38 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plaidoyer: 100%|██████████| 38/38 [00:00<00:00, 109.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RH (40 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RH: 100%|██████████| 40/40 [00:00<00:00, 113.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Éléments envoyés (5559 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Éléments envoyés: 100%|██████████| 5559/5559 [00:49<00:00, 111.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Éléments supprimés (277 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Éléments supprimés: 100%|██████████| 277/277 [00:03<00:00, 92.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Database saved to emails.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def decode_str(s):\n",
    "    \"\"\"Decode encoded email header strings\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        decoded_parts = decode_header(s)\n",
    "        return ''.join([\n",
    "            part.decode(encoding or 'utf-8', errors='replace') if isinstance(part, bytes) else part\n",
    "            for part, encoding in decoded_parts\n",
    "        ])\n",
    "    except:\n",
    "        return str(s)\n",
    "\n",
    "def extract_text_from_html(html_content):\n",
    "    \"\"\"Extract readable text from HTML content without BeautifulSoup\"\"\"\n",
    "    if not html_content:\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Remove HTML tags with regex (simple approach)\n",
    "        text = re.sub(r'<[^>]+>', ' ', html_content)\n",
    "\n",
    "        # Decode HTML entities\n",
    "        text = html.unescape(text)\n",
    "\n",
    "        # Clean up whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "    except:\n",
    "        return html_content  # Return original if parsing fails\n",
    "\n",
    "def get_email_body(message):\n",
    "    \"\"\"Extract body text from email message, handling HTML correctly\"\"\"\n",
    "    body_text = \"\"\n",
    "    body_html = \"\"\n",
    "\n",
    "    if message.is_multipart():\n",
    "        for part in message.walk():\n",
    "            content_type = part.get_content_type()\n",
    "            content_disposition = str(part.get(\"Content-Disposition\") or \"\")\n",
    "\n",
    "            # Skip attachments\n",
    "            if \"attachment\" in content_disposition:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                payload = part.get_payload(decode=True)\n",
    "                if payload is None:\n",
    "                    continue\n",
    "\n",
    "                charset = part.get_content_charset() or 'utf-8'\n",
    "                decoded_payload = payload.decode(charset, errors='replace')\n",
    "\n",
    "                if content_type == \"text/plain\":\n",
    "                    body_text += decoded_payload\n",
    "                elif content_type == \"text/html\":\n",
    "                    body_html += decoded_payload\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        # Not multipart - get payload directly\n",
    "        try:\n",
    "            content_type = message.get_content_type()\n",
    "            payload = message.get_payload(decode=True)\n",
    "            if payload:\n",
    "                charset = message.get_content_charset() or 'utf-8'\n",
    "                decoded_payload = payload.decode(charset, errors='replace')\n",
    "\n",
    "                if content_type == \"text/plain\":\n",
    "                    body_text = decoded_payload\n",
    "                elif content_type == \"text/html\":\n",
    "                    body_html = decoded_payload\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Prefer HTML content but fall back to plain text\n",
    "    if body_html:\n",
    "        return {\n",
    "            \"html\": body_html,\n",
    "            \"text\": extract_text_from_html(body_html),\n",
    "            \"has_html\": True\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"html\": \"\",\n",
    "            \"text\": body_text,\n",
    "            \"has_html\": False\n",
    "        }\n",
    "\n",
    "def extract_attachments_info(message):\n",
    "    \"\"\"Extract information about attachments\"\"\"\n",
    "    attachments = []\n",
    "\n",
    "    if message.is_multipart():\n",
    "        for part in message.walk():\n",
    "            content_disposition = str(part.get(\"Content-Disposition\") or \"\")\n",
    "\n",
    "            if \"attachment\" in content_disposition:\n",
    "                filename = part.get_filename()\n",
    "                if filename:\n",
    "                    try:\n",
    "                        filename = decode_str(filename)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                content_type = part.get_content_type()\n",
    "                attachments.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"content_type\": content_type,\n",
    "                    \"size\": len(part.get_payload(decode=True) or b'')\n",
    "                })\n",
    "\n",
    "    return attachments\n",
    "\n",
    "def extract_recipients(message):\n",
    "    \"\"\"Extract all recipients (To, CC, BCC)\"\"\"\n",
    "    to = decode_str(message.get('to') or \"\")\n",
    "    cc = decode_str(message.get('cc') or \"\")\n",
    "    bcc = decode_str(message.get('bcc') or \"\")\n",
    "\n",
    "    return {\n",
    "        \"to\": to,\n",
    "        \"cc\": cc,\n",
    "        \"bcc\": bcc\n",
    "    }\n",
    "\n",
    "def extract_message_data(message, folder_name):\n",
    "    \"\"\"Extract comprehensive email data\"\"\"\n",
    "    # Extract basic headers\n",
    "    subject = decode_str(message.get('subject') or \"\")\n",
    "    from_addr = decode_str(message.get('from') or \"\")\n",
    "    date_str = message.get('date')\n",
    "    message_id = decode_str(message.get('message-id') or \"\")\n",
    "\n",
    "    # Parse date\n",
    "    try:\n",
    "        date = email.utils.parsedate_to_datetime(date_str)\n",
    "    except:\n",
    "        date = None\n",
    "\n",
    "    # Get recipients\n",
    "    recipients = extract_recipients(message)\n",
    "\n",
    "    # Get body content\n",
    "    body_content = get_email_body(message)\n",
    "\n",
    "    # Get attachment info\n",
    "    attachments = extract_attachments_info(message)\n",
    "\n",
    "    # Thread information\n",
    "    references = decode_str(message.get('references') or \"\")\n",
    "    in_reply_to = decode_str(message.get('in-reply-to') or \"\")\n",
    "\n",
    "    return {\n",
    "        'message_id': message_id,\n",
    "        'subject': subject,\n",
    "        'from': from_addr,\n",
    "        'to': recipients[\"to\"],\n",
    "        'cc': recipients[\"cc\"],\n",
    "        'bcc': recipients[\"bcc\"],\n",
    "        'date': date,\n",
    "        'folder': folder_name,\n",
    "        'body_text': body_content[\"text\"],\n",
    "        'body_html': body_content[\"html\"],\n",
    "        'has_html': body_content[\"has_html\"],\n",
    "        'attachments': attachments,\n",
    "        'attachment_count': len(attachments),\n",
    "        'references': references,\n",
    "        'in_reply_to': in_reply_to\n",
    "    }\n",
    "\n",
    "def process_mbox_to_sqlite(mbox_path, conn, batch_size=100):\n",
    "    \"\"\"Process mbox file directly to SQLite in batches with progress bar\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    mbox = mailbox.mbox(mbox_path)\n",
    "    folder_name = os.path.basename(mbox_path).replace('.mbox', '')\n",
    "\n",
    "    # Count messages for progress bar\n",
    "    total_messages = len(mbox)\n",
    "    print(f\"Processing {folder_name} ({total_messages} messages)\")\n",
    "\n",
    "    # Process in batches\n",
    "    batch = []\n",
    "    for i, message in enumerate(tqdm(mbox, total=total_messages, desc=folder_name)):\n",
    "        email_data = extract_message_data(message, folder_name)\n",
    "\n",
    "        # Convert attachment data to string\n",
    "        if email_data['attachments']:\n",
    "            email_data['attachments_json'] = str(email_data['attachments'])\n",
    "        else:\n",
    "            email_data['attachments_json'] = \"\"\n",
    "\n",
    "        del email_data['attachments']  # Remove original list\n",
    "\n",
    "        batch.append(email_data)\n",
    "\n",
    "        # Process batch\n",
    "        if len(batch) >= batch_size or i == total_messages - 1:\n",
    "            if batch:\n",
    "                # Convert to DataFrame for easy SQL insertion\n",
    "                df = pd.DataFrame(batch)\n",
    "\n",
    "                # Write to database\n",
    "                df.to_sql('emails', conn, if_exists='append', index=False)\n",
    "\n",
    "                # Clear batch\n",
    "                batch = []\n",
    "\n",
    "                # Commit to save progress\n",
    "                conn.commit()\n",
    "\n",
    "def setup_database(db_path):\n",
    "    \"\"\"Set up the SQLite database schema with proper types and indexes\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Drop table if exists\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS emails\")\n",
    "\n",
    "    # Create table with proper columns and types\n",
    "    # Note: Escaping the reserved keyword 'references' with quotes\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE emails (\n",
    "        message_id TEXT,\n",
    "        subject TEXT,\n",
    "        \"from\" TEXT,\n",
    "        \"to\" TEXT,\n",
    "        cc TEXT,\n",
    "        bcc TEXT,\n",
    "        date TIMESTAMP,\n",
    "        folder TEXT,\n",
    "        body_text TEXT,\n",
    "        body_html TEXT,\n",
    "        has_html BOOLEAN,\n",
    "        attachments_json TEXT,\n",
    "        attachment_count INTEGER,\n",
    "        \"references\" TEXT,\n",
    "        in_reply_to TEXT\n",
    "    )\n",
    "    ''')\n",
    "\n",
    "    # Create indexes for common queries\n",
    "    cursor.execute('CREATE INDEX idx_date ON emails(date)')\n",
    "    cursor.execute('CREATE INDEX idx_folder ON emails(folder)')\n",
    "    cursor.execute('CREATE INDEX idx_from ON emails(\"from\")')\n",
    "    cursor.execute('CREATE INDEX idx_to ON emails(\"to\")')\n",
    "    cursor.execute('CREATE INDEX idx_subject ON emails(subject)')\n",
    "\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def process_all_mbox_files(directory, db_path='emails.db'):\n",
    "    \"\"\"Process all mbox files in directory to a SQLite database\"\"\"\n",
    "    # Setup database\n",
    "    conn = setup_database(db_path)\n",
    "\n",
    "    # Get list of mbox files\n",
    "    mbox_files = [f for f in os.listdir(directory) if f.endswith('.mbox')]\n",
    "    print(f\"Found {len(mbox_files)} mbox files\")\n",
    "\n",
    "    # Process each file\n",
    "    for filename in mbox_files:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        process_mbox_to_sqlite(file_path, conn)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Processing complete. Database saved to {db_path}\")\n",
    "\n",
    "# Example usage:\n",
    "directory = \"data/processed/mailbox_cecile/\"\n",
    "process_all_mbox_files(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2128f98",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc74b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connect to the database\n",
    "db_path = \"emails.db\"  # Update this path if needed\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Method 1: Get the entire table as a DataFrame\n",
    "df = pd.read_sql_query(\"SELECT * FROM emails\", conn)\n",
    "\n",
    "# If the table is very large, you might want to limit it\n",
    "# df = pd.read_sql_query(\"SELECT * FROM emails LIMIT 1000\", conn)\n",
    "\n",
    "# Method 2: Get specific columns only\n",
    "# columns_df = pd.read_sql_query(\"SELECT message_id, subject, \\\"from\\\", \\\"to\\\", date, folder FROM emails\", conn)\n",
    "\n",
    "# # Method 3: With a specific condition\n",
    "# filtered_df = pd.read_sql_query(\"SELECT * FROM emails WHERE folder = 'Inbox'\", conn)\n",
    "\n",
    "# Don't forget to close the connection when done\n",
    "conn.close()\n",
    "\n",
    "# Now you can work with the DataFrame(s)\n",
    "print(df.shape)  # Shows number of rows and columns\n",
    "print(df.columns)  # Shows all column names\n",
    "df.head(2)  # Shows the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d42453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import html\n",
    "# from tqdm import tqdm\n",
    "# import sys\n",
    "# import argparse\n",
    "\n",
    "# def extract_clean_text_from_html(html_content):\n",
    "#     \"\"\"\n",
    "#     Extract clean, readable text from HTML content.\n",
    "\n",
    "#     Args:\n",
    "#         html_content (str): HTML content to clean\n",
    "\n",
    "#     Returns:\n",
    "#         str: Clean text without HTML tags\n",
    "#     \"\"\"\n",
    "#     if not html_content:\n",
    "#         return \"\"\n",
    "\n",
    "#     try:\n",
    "#         # Remove scripts, styles, and other tags that contain content we don't want\n",
    "#         html_content = re.sub(r'<(script|style|head).*?>.*?</\\1>', ' ', html_content, flags=re.DOTALL)\n",
    "\n",
    "#         # Replace common block elements with newlines to preserve structure\n",
    "#         html_content = re.sub(r'</(p|div|h\\d|tr|li)>', '\\n', html_content)\n",
    "#         html_content = re.sub(r'<br[^>]*>', '\\n', html_content)\n",
    "\n",
    "#         # Replace table cells with tab separation\n",
    "#         html_content = re.sub(r'</td>', '\\t', html_content)\n",
    "\n",
    "#         # Remove all HTML tags\n",
    "#         text = re.sub(r'<[^>]+>', ' ', html_content)\n",
    "\n",
    "#         # Decode HTML entities (&nbsp;, &lt;, etc.)\n",
    "#         text = html.unescape(text)\n",
    "\n",
    "#         # Fix some common patterns in emails\n",
    "#         text = re.sub(r'De :\\s+.*?Envoyé :', '\\n---\\nDe : ', text)\n",
    "\n",
    "#         # Clean up whitespace (multiple spaces, tabs, newlines)\n",
    "#         text = re.sub(r'[ \\t]+', ' ', text)\n",
    "#         text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "#         # Final cleanup to remove leading/trailing whitespace\n",
    "#         return text.strip()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing HTML: {e}\")\n",
    "#         return f\"Error processing HTML content: {str(e)}\"\n",
    "\n",
    "# def process_database(db_path, limit=None, batch_size=500, verbose=True):\n",
    "#     \"\"\"Process the entire database, adding clean text for all HTML emails\"\"\"\n",
    "#     conn = sqlite3.connect(db_path)\n",
    "#     cursor = conn.cursor()\n",
    "\n",
    "#     # Add column if it doesn't exist\n",
    "#     try:\n",
    "#         cursor.execute(\"ALTER TABLE emails ADD COLUMN body_clean_text TEXT\")\n",
    "#         print(\"Added body_clean_text column to database\")\n",
    "#     except sqlite3.OperationalError:\n",
    "#         if verbose:\n",
    "#             print(\"The body_clean_text column already exists\")\n",
    "\n",
    "#     # Count HTML emails\n",
    "#     cursor.execute(\"SELECT COUNT(*) FROM emails WHERE has_html = 1\")\n",
    "#     total_html = cursor.fetchone()[0]\n",
    "\n",
    "#     if limit:\n",
    "#         total_to_process = min(limit, total_html)\n",
    "#     else:\n",
    "#         total_to_process = total_html\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"Processing {total_to_process} of {total_html} HTML emails...\")\n",
    "\n",
    "#     # Process HTML emails in batches\n",
    "#     processed = 0\n",
    "#     for offset in range(0, total_to_process, batch_size):\n",
    "#         if limit and offset >= limit:\n",
    "#             break\n",
    "\n",
    "#         current_batch = min(batch_size, total_to_process - offset)\n",
    "\n",
    "#         if verbose:\n",
    "#             print(f\"Processing batch {offset//batch_size + 1} ({offset+1}-{offset+current_batch} of {total_to_process})\")\n",
    "\n",
    "#         # Get batch of emails\n",
    "#         cursor.execute(f\"\"\"\n",
    "#             SELECT rowid, message_id, body_html\n",
    "#             FROM emails\n",
    "#             WHERE has_html = 1\n",
    "#             LIMIT {current_batch} OFFSET {offset}\n",
    "#         \"\"\")\n",
    "\n",
    "#         rows = cursor.fetchall()\n",
    "\n",
    "#         # Process each email in the batch\n",
    "#         for row in tqdm(rows, desc=\"Processing\", disable=not verbose):\n",
    "#             rowid, message_id, body_html = row\n",
    "\n",
    "#             if body_html:\n",
    "#                 clean_text = extract_clean_text_from_html(body_html)\n",
    "\n",
    "#                 # Update database\n",
    "#                 cursor.execute(\n",
    "#                     \"UPDATE emails SET body_clean_text = ? WHERE rowid = ?\",\n",
    "#                     (clean_text, rowid)\n",
    "#                 )\n",
    "\n",
    "#                 processed += 1\n",
    "\n",
    "#         # Commit after each batch\n",
    "#         conn.commit()\n",
    "\n",
    "#     # Process non-HTML emails (copy body_text to body_clean_text)\n",
    "#     cursor.execute(\"SELECT COUNT(*) FROM emails WHERE has_html = 0 AND body_clean_text IS NULL\")\n",
    "#     non_html_count = cursor.fetchone()[0]\n",
    "\n",
    "#     if non_html_count > 0:\n",
    "#         if verbose:\n",
    "#             print(f\"Copying text for {non_html_count} non-HTML emails...\")\n",
    "\n",
    "#         cursor.execute(\"\"\"\n",
    "#             UPDATE emails\n",
    "#             SET body_clean_text = body_text\n",
    "#             WHERE has_html = 0 AND body_clean_text IS NULL\n",
    "#         \"\"\")\n",
    "#         conn.commit()\n",
    "\n",
    "#     #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b215e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html xmlns:v=\"urn:schemas-microsoft-com:vml\" xmlns:o=\"urn:schemas-microsoft-com:office:office\" xmlns:w=\"urn:schemas-microsoft-com:office:word\" xmlns:m=\"http://schemas.microsoft.com/office/2004/12/omml\" xmlns=\"http://www.w3.org/TR/REC-html40\"><head><meta http-equiv=Content-Type content=\"text/html; charset=iso-8859-1\"><meta name=Generator content=\"Microsoft Word 15 (filtered medium)\"><!--[if !mso]><style>v\\\\:* {behavior:url(#default#VML);}\\no\\\\:* {behavior:url(#default#VML);}\\nw\\\\:* {behavior:url(#default#VML);}\\n.shape {behavior:url(#default#VML);}\\n</style><![endif]--><style><!--\\n/* Font Definitions */\\n@font-face\\n\\t{font-family:Helvetica;\\n\\tpanose-1:2 11 6 4 2 2 2 2 2 4;}\\n@font-face\\n\\t{font-family:\"Cambria Math\";\\n\\tpanose-1:2 4 5 3 5 4 6 3 2 4;}\\n@font-face\\n\\t{font-family:Calibri;\\n\\tpanose-1:2 15 5 2 2 2 4 3 2 4;}\\n@font-face\\n\\t{font-family:\"Century Gothic\";\\n\\tpanose-1:2 11 5 2 2 2 2 2 2 4;}\\n@font-face\\n\\t{font-family:\"HelveticaNeueLT Com 55 Roman\";}\\n@font-face\\n\\t{font-family:Effra;}\\n/* Style Definitions */\\np.MsoNormal, li.MsoNormal, div.MsoNormal\\n\\t{margin:0cm;\\n\\tmargin-bottom:.0001pt;\\n\\tfont-size:11.0pt;\\n\\tfont-family:\"Calibri\",sans-serif;\\n\\tmso-fareast-language:EN-US;}\\na:link, span.MsoHyperlink\\n\\t{mso-style-priority:99;\\n\\tcolor:#0563C1;\\n\\ttext-decoration:underline;}\\nspan.EmailStyle21\\n\\t{mso-style-type:personal-reply;\\n\\tfont-family:\"Calibri\",sans-serif;\\n\\tcolor:windowtext;}\\n.MsoChpDefault\\n\\t{mso-style-type:export-only;\\n\\tfont-size:10.0pt;}\\n@page WordSection1\\n\\t{size:612.0pt 792.0pt;\\n\\tmargin:70.85pt 70.85pt 70.85pt 70.85pt;}\\ndiv.WordSection1\\n\\t{page:WordSection1;}\\n--></style><!--[if gte mso 9]><xml>\\n<o:shapedefaults v:ext=\"edit\" spidmax=\"1028\" />\\n</xml><![endif]--><!--[if gte mso 9]><xml>\\n<o:shapelayout v:ext=\"edit\">\\n<o:idmap v:ext=\"edit\" data=\"1\" />\\n</o:shapelayout></xml><![endif]--></head><body lang=FR link=\"#0563C1\" vlink=\"#954F72\"><div class=WordSection1><p class=MsoNormal>Bonjour à tous, <o:p></o:p></p><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal>J&#8217;ai invité Mr Buchoux à notre AG et il sera présent.<o:p></o:p></p><p class=MsoNormal><br>Cordialement<o:p></o:p></p><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal><!--[if gte vml 1]><v:shapetype id=\"_x0000_t75\" coordsize=\"21600,21600\" o:spt=\"75\" o:preferrelative=\"t\" path=\"m@4@5l@4@11@9@11@9@5xe\" filled=\"f\" stroked=\"f\">\\n<v:stroke joinstyle=\"miter\" />\\n<v:formulas>\\n<v:f eqn=\"if lineDrawn pixelLineWidth 0\" />\\n<v:f eqn=\"sum @0 1 0\" />\\n<v:f eqn=\"sum 0 0 @1\" />\\n<v:f eqn=\"prod @2 1 2\" />\\n<v:f eqn=\"prod @3 21600 pixelWidth\" />\\n<v:f eqn=\"prod @3 21600 pixelHeight\" />\\n<v:f eqn=\"sum @0 0 1\" />\\n<v:f eqn=\"prod @6 1 2\" />\\n<v:f eqn=\"prod @7 21600 pixelWidth\" />\\n<v:f eqn=\"sum @8 21600 0\" />\\n<v:f eqn=\"prod @7 21600 pixelHeight\" />\\n<v:f eqn=\"sum @10 21600 0\" />\\n</v:formulas>\\n<v:path o:extrusionok=\"f\" gradientshapeok=\"t\" o:connecttype=\"rect\" />\\n<o:lock v:ext=\"edit\" aspectratio=\"t\" />\\n</v:shapetype><v:shape id=\"Image_x0020_1\" o:spid=\"_x0000_s1027\" type=\"#_x0000_t75\" style=\\'position:absolute;margin-left:-4.85pt;margin-top:.25pt;width:107.5pt;height:150.7pt;z-index:251660288;visibility:visible;mso-wrap-style:square;mso-width-percent:0;mso-height-percent:0;mso-wrap-distance-left:9pt;mso-wrap-distance-top:0;mso-wrap-distance-right:9pt;mso-wrap-distance-bottom:0;mso-position-horizontal:absolute;mso-position-horizontal-relative:text;mso-position-vertical:absolute;mso-position-vertical-relative:text;mso-width-percent:0;mso-height-percent:0;mso-width-relative:page;mso-height-relative:page\\'>\\n<v:imagedata src=\"cid:image006.jpg@01D63F43.70FD3750\" o:title=\"\" />\\n<w:wrap type=\"square\"/>\\n</v:shape><![endif]--><![if !vml]><img width=143 height=201 style=\\'width:1.4895in;height:2.0937in\\' src=\"cid:image007.jpg@01D63F43.70FD3750\" align=left hspace=12 v:shapes=\"Image_x0020_1\"><![endif]><b><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:#9DC107;mso-fareast-language:FR\\'><o:p></o:p></span></b></p><p class=MsoNormal><b><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:#9DC107;mso-fareast-language:FR\\'>Anne Clerc<o:p></o:p></span></b></p><p class=MsoNormal><b><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:#9DC107;mso-fareast-language:FR\\'>Déléguée Générale<o:p></o:p></span></b></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR\\'>Association des archivistes français<o:p></o:p></span></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR\\'>8 rue Jean-Marie Jégo<o:p></o:p></span></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR\\'>75013 PARIS<o:p></o:p></span></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR\\'>Tél. 01 46 06 40 12<o:p></o:p></span></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR\\'>Portable 06 79 53 47 40<o:p></o:p></span></p><p class=MsoNormal><span style=\\'mso-fareast-language:FR\\'><a href=\"mailto:vieassociative@archivistes.org\"><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:blue\\'>delegation_generale@archivistes.org</span></a></span><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR\\'><o:p></o:p></span></p><p class=MsoNormal><span style=\\'mso-fareast-language:FR\\'><a href=\"https://www.archivistes.org/\"><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:blue\\'>https://www.archivistes.org</span></a></span><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR\\'><o:p></o:p></span></p><p class=MsoNormal><b><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:#9DC107;mso-fareast-language:FR\\'><o:p>&nbsp;</o:p></span></b></p><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal><o:p>&nbsp;</o:p></p><div><div style=\\'border:none;border-top:solid #E1E1E1 1.0pt;padding:3.0pt 0cm 0cm 0cm\\'><p class=MsoNormal><b><span style=\\'mso-fareast-language:FR\\'>De&nbsp;:</span></b><span style=\\'mso-fareast-language:FR\\'> Daniel BUCHOUX &lt;d.buchoux@aca.nexia.fr&gt; <br><b>Envoyé&nbsp;:</b> mercredi 10 juin 2020 16:17<br><b>À&nbsp;:</b> AAF - Anne Clerc, déléguée générale &lt;delegation_generale@archivistes.org&gt;; Dolly RAZAFINANJA &lt;d.razafinanja@aca.nexia.fr&gt;; Simon DELEDICQ &lt;s.deledicq@aca.nexia.fr&gt;<br><b>Objet&nbsp;:</b> RE: Rappel : Assemblée générale de l\\'AAF le 29 juin 2020 à partir de 17h en visioconférence<o:p></o:p></span></p></div></div><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:Effra;color:#575756\\'>Bonjour Anne,<o:p></o:p></span></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:Effra;color:#575756\\'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:Effra;color:#575756\\'>J&#8217;espère que vous allez bien.<o:p></o:p></span></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:Effra;color:#575756\\'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:Effra;color:#575756\\'>Je serai présent.<o:p></o:p></span></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:Effra;color:#575756\\'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:Effra;color:#575756\\'>Bonne journée.<o:p></o:p></span></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:Effra;color:#575756\\'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><b><span style=\\'font-size:10.0pt;font-family:Effra;color:#00B2A9;mso-fareast-language:FR\\'>Daniel Buchoux </span></b><span style=\\'font-size:9.0pt;font-family:Effra;color:#575756;mso-fareast-language:FR\\'><br><br>Aca Nexia </span><b><span style=\\'font-size:9.0pt;font-family:Effra;color:#00B2A9;mso-fareast-language:FR\\'>|</span></b><span style=\\'font-size:9.0pt;font-family:Effra;color:#575756;mso-fareast-language:FR\\'> 31, rue Henri Rochefort, 75017 Paris, France <br></span><b><span style=\\'font-size:9.0pt;font-family:Effra;color:#00B2A9;mso-fareast-language:FR\\'>T</span></b><span style=\\'font-size:9.0pt;font-family:Effra;color:#575756;mso-fareast-language:FR\\'> +33 (0)1 47 66 77 88 </span><b><span style=\\'font-size:9.0pt;font-family:Effra;color:#00B2A9;mso-fareast-language:FR\\'>| M</span></b><span style=\\'font-size:9.0pt;font-family:Effra;color:#575756;mso-fareast-language:FR\\'> +33 (0)6 07 76 25 49 <br></span><b><span style=\\'font-size:9.0pt;font-family:Effra;color:#00B2A9;mso-fareast-language:FR\\'>E</span></b><span style=\\'font-size:9.0pt;font-family:Effra;color:#575756;mso-fareast-language:FR\\'> <a href=\"mailto:d.buchoux@aca.nexia.fr\"><span style=\\'color:#575756\\'>d.buchoux@aca.nexia.fr</span></a> <br><a href=\"http://www.aca.nexia.fr\"><span style=\\'color:#575756\\'>aca.nexia.fr</span></a> <br><br><img border=0 width=100 height=39 style=\\'width:1.0416in;height:.4062in\\' id=\"Image_x0020_9\" src=\"cid:image008.png@01D63F43.70FD3750\" alt=\"Aca, Independant member of Nexia International\">&nbsp; </span><span style=\\'font-size:10.0pt;font-family:Effra;color:#575756\\'><o:p></o:p></span></p><div><div style=\\'border:none;border-top:solid #E1E1E1 1.0pt;padding:3.0pt 0cm 0cm 0cm\\'><p class=MsoNormal><b><span style=\\'mso-fareast-language:FR\\'>De&nbsp;:</span></b><span style=\\'mso-fareast-language:FR\\'> AAF - Anne Clerc, déléguée générale &lt;delegation_generale@archivistes.org&gt; <br><b>Envoyé&nbsp;:</b> mercredi 10 juin 2020 15:53<br><b>À&nbsp;:</b> Dolly RAZAFINANJA &lt;d.razafinanja@aca.nexia.fr&gt;; Daniel BUCHOUX &lt;d.buchoux@aca.nexia.fr&gt;; Simon DELEDICQ &lt;s.deledicq@aca.nexia.fr&gt;<br><b>Objet&nbsp;:</b> TR: Rappel : Assemblée générale de l\\'AAF le 29 juin 2020 à partir de 17h en visioconférence<o:p></o:p></span></p></div></div><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal>Bonjour, <o:p></o:p></p><p class=MsoNormal>&nbsp;<o:p></o:p></p><p class=MsoNormal>Pour information, je vous transmets l&#8217;invitation pour notre AG. En comptant sur votre présence.<o:p></o:p></p><p class=MsoNormal><br>Cordialement<o:p></o:p></p><p class=MsoNormal>&nbsp;<o:p></o:p></p><p class=MsoNormal>&nbsp;<o:p></o:p></p><p class=MsoNormal><!--[if gte vml 1]><v:shape id=\"Image_x0020_2\" o:spid=\"_x0000_s1026\" type=\"#_x0000_t75\" style=\\'position:absolute;margin-left:0;margin-top:0;width:107.25pt;height:150.75pt;z-index:251658240;visibility:visible;mso-wrap-style:square;mso-width-percent:0;mso-height-percent:0;mso-wrap-distance-left:4.5pt;mso-wrap-distance-top:0;mso-wrap-distance-right:4.5pt;mso-wrap-distance-bottom:0;mso-position-horizontal:left;mso-position-horizontal-relative:text;mso-position-vertical:absolute;mso-position-vertical-relative:line;mso-width-percent:0;mso-height-percent:0;mso-width-relative:page;mso-height-relative:page\\' o:allowoverlap=\"f\">\\n<v:imagedata src=\"cid:image002.jpg@01D63F42.AB84B200\" o:title=\"\" />\\n<w:wrap type=\"square\" anchory=\"line\"/>\\n</v:shape><![endif]--><![if !vml]><img width=143 height=201 style=\\'width:1.4895in;height:2.0937in\\' src=\"cid:image002.jpg@01D63F42.AB84B200\" align=left hspace=6 v:shapes=\"Image_x0020_2\"><![endif]><b><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:#9DC107;mso-fareast-language:FR\\'>Anne Clerc</span></b><o:p></o:p></p><p class=MsoNormal><b><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:#9DC107;mso-fareast-language:FR\\'>Déléguée Générale</span></b><o:p></o:p></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR\\'>Association des archivistes français</span><o:p></o:p></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR\\'>8 rue Jean-Marie Jégo</span><o:p></o:p></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR\\'>75013 PARIS</span><o:p></o:p></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR\\'>Tél. 01 46 06 40 12</span><o:p></o:p></p><p class=MsoNormal><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";mso-fareast-language:FR\\'>Portable 06 79 53 47 40</span><o:p></o:p></p><p class=MsoNormal><span style=\\'mso-fareast-language:FR\\'><a href=\"mailto:vieassociative@archivistes.org\"><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:blue\\'>delegation_generale@archivistes.org</span></a></span><o:p></o:p></p><p class=MsoNormal><span style=\\'mso-fareast-language:FR\\'><a href=\"https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.archivistes.org%2F&amp;data=01%7C01%7Cd.buchoux%40aca.nexia.fr%7C8fa859003d0746647f4b08d80d459662%7Cdacd1544451646239649a9e24a9ec751%7C0&amp;sdata=POSAcM96sIcLAw6%2BHo2YN9YeaS%2F0erF2hwUGSQWXiGs%3D&amp;reserved=0\"><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:blue\\'>https://www.archivistes.org</span></a></span><o:p></o:p></p><p class=MsoNormal><b><span style=\\'font-size:10.0pt;font-family:\"HelveticaNeueLT Com 55 Roman\";color:#9DC107;mso-fareast-language:FR\\'>&nbsp;</span></b><o:p></o:p></p><p class=MsoNormal>&nbsp;<o:p></o:p></p><p class=MsoNormal>&nbsp;<o:p></o:p></p><p class=MsoNormal>&nbsp;<o:p></o:p></p><div><div style=\\'border:none;border-top:solid #E1E1E1 1.0pt;padding:3.0pt 0cm 0cm 0cm\\'><p class=MsoNormal><b><span style=\\'mso-fareast-language:FR\\'>De&nbsp;:</span></b><span style=\\'mso-fareast-language:FR\\'> <a href=\"mailto:aaf_adherents-request@listes.archivistes.org\">aaf_adherents-request@listes.archivistes.org</a> &lt;<a href=\"mailto:aaf_adherents-request@listes.archivistes.org\">aaf_adherents-request@listes.archivistes.org</a>&gt; <b>De la part de</b> AAF vie associative - Nicolas Didon<br><b>Envoyé&nbsp;:</b> mercredi 10 juin 2020 08:54<br><b>À&nbsp;:</b> <a href=\"mailto:aaf_adherents@listes.archivistes.org\">aaf_adherents@listes.archivistes.org</a><br><b>Objet&nbsp;:</b> [aaf_adherents] Rappel : Assemblée générale de l\\'AAF le 29 juin 2020 à partir de 17h en visioconférence</span><o:p></o:p></p></div></div><p class=MsoNormal>&nbsp;<o:p></o:p></p><p class=MsoNormal><b><i><u><span style=\\'font-family:\"Helvetica\",sans-serif\\'>Aux membres de l&#8217;AAF</span></u></i></b><o:p></o:p></p><p class=MsoNormal><img border=0 width=516 height=258 style=\\'width:5.375in;height:2.6875in\\' id=\"_x0000_i1032\" src=\"cid:image003.png@01D63F42.AB84B200\"><o:p></o:p></p><table class=MsoNormalTable border=0 cellspacing=0 cellpadding=0 width=\"100%\" style=\\'width:100.0%;border-collapse:collapse\\'><tr><td width=\"100%\" style=\\'width:100.0%;padding:3.75pt 7.5pt 3.75pt 7.5pt\\'><p class=MsoNormal><span style=\\'font-family:\"Helvetica\",sans-serif\\'>Cher.e membre,<br><br>J&#8217;ai l&#8217;hon\\xadneur de vous infor\\xadmer que l&#8217;assem\\xadblée géné\\xadrale ordi\\xadnaire annuelle de notre asso\\xadcia\\xadtion se tien\\xaddra le&nbsp;:<br><br><strong><span style=\\'font-family:\"Helvetica\",sans-serif\\'>Lundi 29 juin 2020 de 17h à 19h en visio\\xadconfé\\xadrence </span></strong><br><br>Initialement prévue le 20 mars, l&#8217;assem\\xadblée géné\\xadrale a été repor\\xadtée en raison du contexte sani\\xadtaire.<br><br>L&#8217;ordre du jour de l&#8217;assem\\xadblée géné\\xadrale est le sui\\xadvant&nbsp;:<br><span style=\\'border:solid windowtext 1.0pt;padding:0cm\\'><img border=0 width=9 height=9 style=\\'width:.0937in;height:.0937in\\' id=\"Image_x0020_1\" src=\"cid:image004.jpg@01D63F42.AB84B200\" alt=\"Image supprimée par l\\'expéditeur. -\"></span>&nbsp;Approbation du procès-verbal de la pré\\xadcé\\xaddente réu\\xadnion d&#8217;avril 2019&nbsp;;<br><span style=\\'border:solid windowtext 1.0pt;padding:0cm\\'><img border=0 width=9 height=9 style=\\'width:.0937in;height:.0937in\\' id=\"Image_x0020_2\" src=\"cid:image004.jpg@01D63F42.AB84B200\" alt=\"Image supprimée par l\\'expéditeur. -\"></span>&nbsp;Approbation du rap\\xadport moral de la pré\\xadsi\\xaddente pour l&#8217;année 2019&nbsp;;<br><span style=\\'border:solid windowtext 1.0pt;padding:0cm\\'><img border=0 width=9 height=9 style=\\'width:.0937in;height:.0937in\\' id=\"Image_x0020_3\" src=\"cid:image004.jpg@01D63F42.AB84B200\" alt=\"Image supprimée par l\\'expéditeur. -\"></span>&nbsp;Approbation du rap\\xadport finan\\xadcier de la tré\\xadso\\xadrière, appro\\xadba\\xadtion des comp\\xadtes de l&#8217;exer\\xadcice clos le 31 décem\\xadbre 2019&nbsp;;<br><span style=\\'border:solid windowtext 1.0pt;padding:0cm\\'><img border=0 width=9 height=9 style=\\'width:.0937in;height:.0937in\\' id=\"Image_x0020_4\" src=\"cid:image004.jpg@01D63F42.AB84B200\" alt=\"Image supprimée par l\\'expéditeur. -\"></span>&nbsp;Présentation des comp\\xadtes de l&#8217;exer\\xadcice clos le 31 décem\\xadbre 2019 pour la filiale Archivistes Français Formation&nbsp;;<br><span style=\\'border:solid windowtext 1.0pt;padding:0cm\\'><img border=0 width=9 height=9 style=\\'width:.0937in;height:.0937in\\' id=\"Image_x0020_5\" src=\"cid:image004.jpg@01D63F42.AB84B200\" alt=\"Image supprimée par l\\'expéditeur. -\"></span>&nbsp;Présentation et appro\\xadba\\xadtion du budget 2020 révisé pour l&#8217;AAF&nbsp;;<br><span style=\\'border:solid windowtext 1.0pt;padding:0cm\\'><img border=0 width=9 height=9 style=\\'width:.0937in;height:.0937in\\' id=\"Image_x0020_6\" src=\"cid:image004.jpg@01D63F42.AB84B200\" alt=\"Image supprimée par l\\'expéditeur. -\"></span>&nbsp;Présentation du budget 2020 révisé pour la filiale Archivistes Français Formation&nbsp;;<br><span style=\\'border:solid windowtext 1.0pt;padding:0cm\\'><img border=0 width=9 height=9 style=\\'width:.0937in;height:.0937in\\' id=\"Image_x0020_7\" src=\"cid:image004.jpg@01D63F42.AB84B200\" alt=\"Image supprimée par l\\'expéditeur. -\"></span>&nbsp;Fixation du mon\\xadtant des coti\\xadsa\\xadtions pour l&#8217;année 2021.<br><br>Je vous invite à pren\\xaddre connais\\xadsance de l&#8217;ensem\\xadble des docu\\xadments et infor\\xadma\\xadtions rela\\xadtifs à l&#8217;acti\\xadvité et à la situa\\xadtion finan\\xadcière de l&#8217;AAF et de sa filiale Archivistes Français Formation (rap\\xadport moral de la pré\\xadsi\\xaddente, rap\\xadport finan\\xadcier et comp\\xadtes de résul\\xadtat, bud\\xadgets pré\\xadvi\\xadsion\\xadnels, rap\\xadports des sec\\xadtions et des comi\\xadtés, mon\\xadtant des coti\\xadsa\\xadtions pour 2021), en ligne, sur le site Internet de l&#8217;asso\\xadcia\\xadtion&nbsp;: [<a href=\"https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.archivistes.org%2F-AG-du-2020_03_20&amp;data=01%7C01%7Cd.buchoux%40aca.nexia.fr%7C8fa859003d0746647f4b08d80d459662%7Cdacd1544451646239649a9e24a9ec751%7C0&amp;sdata=i4xCMSOxhr3LUNKtRDcsvtn4Vvba9UR94e%2B8ZW%2Br6rY%3D&amp;reserved=0\">https://www.archi\\xadvis\\xadtes.org/-AG-du-2020_03_20</a><a href=\"https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.archivistes.org%2F-AG-du-2020_03_20-&amp;data=01%7C01%7Cd.buchoux%40aca.nexia.fr%7C8fa859003d0746647f4b08d80d459662%7Cdacd1544451646239649a9e24a9ec751%7C0&amp;sdata=v%2BK6M3%2BFlhnpoy47RLT0hSeKQOAx%2FjUwFerpwTA4ysE%3D&amp;reserved=0\">-</a>]. Il faut vous connecter avec vos identifiants pour y avoir accès.&nbsp;<br><br><strong><span style=\\'font-family:\"Helvetica\",sans-serif\\'>Pour par\\xadti\\xadci\\xadper à l&#8217;AG en visio\\xadconfé\\xadrence, merci de vous ins\\xadcrire préa\\xadla\\xadble\\xadment ici&nbsp;:&nbsp;</span></strong><br></span><strong><span style=\\'font-size:14.0pt;font-family:\"Helvetica\",sans-serif\\'><a href=\"https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.weezevent.com%2Fparticipation-a-l-ag-2020&amp;data=01%7C01%7Cd.buchoux%40aca.nexia.fr%7C8fa859003d0746647f4b08d80d459662%7Cdacd1544451646239649a9e24a9ec751%7C0&amp;sdata=vvkKHm%2Fw8ok5hzyU6a0OFdd6wZGhOt0Uw4p3qIfyu%2BU%3D&amp;reserved=0\">https://www.wee\\xadze\\xadvent.com/par\\xadti\\xadci\\xadpa\\xadtion-a-l-ag-2020</a> </span></strong><span style=\\'font-family:\"Helvetica\",sans-serif\\'><br><br>Les moda\\xadli\\xadtés de connexion et d&#8217;accès à l&#8217;AG en ligne vous seront trans\\xadmi\\xadses ulté\\xadrieu\\xadre\\xadment.</span><o:p></o:p></p><p class=MsoNormal><span style=\\'font-family:\"Helvetica\",sans-serif\\'><br><br></span><strong><span style=\\'font-size:12.0pt;font-family:\"Helvetica\",sans-serif\\'>Votes&nbsp;</span></strong><strong><span style=\\'font-family:\"Helvetica\",sans-serif\\'>:</span></strong><span style=\\'font-family:\"Helvetica\",sans-serif\\'><br>Les votes se feront en ligne à comp\\xadter du 29 juin, 17h, jusqu&#8217;au 1er juillet, 17h, via un for\\xadmu\\xadlaire qui vous sera com\\xadmu\\xadni\\xadqué à l&#8217;ouver\\xadture de l&#8217;AG. Si vous ne pouvez pas assis\\xadter à cet évènement en direct, vous pour\\xadrez néan\\xadmoins accé\\xadder aux vidéos de pré\\xadsen\\xadta\\xadtion qui seront acces\\xadsi\\xadbles sur notre site, à comp\\xadter du 30 juin au matin, vous per\\xadmet\\xadtant de voter dans les temps impar\\xadtis.</span><o:p></o:p></p><p class=MsoNormal><span style=\\'font-family:\"Helvetica\",sans-serif\\'>&nbsp;</span><o:p></o:p></p><p class=MsoNormal><span style=\\'font-family:\"Helvetica\",sans-serif\\'>Seuls les adhérents individuels et les mandataires des organismes adhérents à jour de cotisation pourront voter et recevront le lien vers le formulaire.</span><o:p></o:p></p><p class=MsoNormal><span style=\\'font-family:\"Helvetica\",sans-serif\\'>&nbsp;</span><o:p></o:p></p><p class=MsoNormal><span style=\\'font-family:\"Helvetica\",sans-serif\\'>Les adhérents individuels doivent être à jour de leurs cotisations 2019 et 2020 (compte tenu cette année de la date de l&#8217;assemblée générale).</span><o:p></o:p></p><p class=MsoNormal><span style=\\'font-family:\"Helvetica\",sans-serif\\'>Si ce n&#8217;est déjà fait, les adhérents individuels sont invités à régler en ligne sur leur Extranet leurs cotisations. </span><o:p></o:p></p><p class=MsoNormal><span style=\\'font-family:\"Helvetica\",sans-serif\\'>Vous avez reçu un mel avec vos identifiants le 28 janvier dernier. Si vous ne le retrouvez pas, n&#8217;hésitez pas à contac\\xadter Nicolas Didon, en charge de la vie asso\\xadcia\\xadtive (<a href=\"mailto:vieassociative@archivistes.org\">vieassociative@archivistes.org</a>).</span><o:p></o:p></p><p class=MsoNormal><span style=\\'font-family:\"Helvetica\",sans-serif\\'><br><br>Comptant sur votre par\\xadti\\xadci\\xadpa\\xadtion, je vous prie d&#8217;agréer, cher.e membre, l&#8217;expres\\xadsion de mes salu\\xadta\\xadtions ami\\xadca\\xadles.<br>&nbsp; </span><o:p></o:p></p><table class=MsoNormalTable border=0 cellspacing=0 cellpadding=0 style=\\'box-sizing: border-box\\'><tr style=\\'box-sizing: border-box\\'><td width=494 style=\\'width:370.5pt;padding:0cm 0cm 0cm 0cm;box-sizing: border-box\\'><p class=MsoNormal><strong><span style=\\'font-family:\"Helvetica\",sans-serif\\'>Céline GUYON</span></strong><span style=\\'font-family:\"Helvetica\",sans-serif\\'><br><strong><span style=\\'font-family:\"Helvetica\",sans-serif\\'>Présidente de l&#8217;Association des archivistes français</span></strong><br>8 rue Jean-Marie Jégo - 75013 Paris<br>Standard :&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 01 46 06 39 44<br><a href=\"mailto:presidence@archivistes.org\" target=\"_blank\">presidence@archivistes.org</a><br><a href=\"https://eur02.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.archivistes.org%2F&amp;data=01%7C01%7Cd.buchoux%40aca.nexia.fr%7C8fa859003d0746647f4b08d80d459662%7Cdacd1544451646239649a9e24a9ec751%7C0&amp;sdata=Y%2Fe5k1uSQP4EpfweRg%2B3N6QKD8UDYf1CGUhNS7%2B0mq0%3D&amp;reserved=0\" target=\"_blank\">www.archivistes.org</a></span><o:p></o:p></p></td></tr></table></td></tr></table><p class=MsoNormal style=\\'mso-margin-top-alt:auto;mso-margin-bottom-alt:auto;background:white\\'><b><span style=\\'font-size:10.0pt;font-family:\"Century Gothic\",sans-serif;color:#00989F;mso-fareast-language:FR\\'>&nbsp;</span></b><o:p></o:p></p><p class=MsoNormal>&nbsp;<o:p></o:p></p></div></body></html>'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"body_html\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ffe9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "1        <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "705      <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "725      <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "727      <html><head>\\n<meta http-equiv=\"Content-Type\" ...\n",
       "1474     <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "1562     <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "1572     <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "5258     <meta http-equiv=\"Content-Type\" content=\"text/...\n",
       "6271     <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "6774     <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "8466     <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "8476     <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "9354     <html>\\n<head>\\n<meta http-equiv=\"Content-Type...\n",
       "9397     <META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/...\n",
       "9410     <META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/...\n",
       "9411     <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "9412     <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "10911    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "10914    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "10915    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "10916    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "10921    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "10947    <META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/...\n",
       "11017    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "11833    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "11851    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "12231    <html>\\n<head>\\n<meta http-equiv=\"Content-Type...\n",
       "12232    <html>\\n<head>\\n<meta http-equiv=\"Content-Type...\n",
       "12253    <html>\\n<head>\\n<meta http-equiv=\"Content-Type...\n",
       "12340    <html><head>\\n<meta http-equiv=\"Content-Type\" ...\n",
       "13410    <html><head>\\n<meta http-equiv=\"Content-Type\" ...\n",
       "13487    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "15785    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "16453    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "17254    <html>\\n<head>\\n<meta http-equiv=\"Content-Type...\n",
       "17256    <html>\\n<head>\\n<meta http-equiv=\"Content-Type...\n",
       "17258    <html>\\n<head>\\n<meta http-equiv=\"Content-Type...\n",
       "17682    <html><head>\\n<meta http-equiv=\"Content-Type\" ...\n",
       "17683    <html><head>\\n<meta http-equiv=\"Content-Type\" ...\n",
       "17684    <html>\\n<head>\\n<meta http-equiv=\"Content-Type...\n",
       "17893    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "17918    <html>\\n<head>\\n<meta http-equiv=\"Content-Type...\n",
       "18316    <html>\\n<head>\\n<meta http-equiv=\"Content-Type...\n",
       "18317    <html>\\n<head>\\n<meta http-equiv=\"Content-Type...\n",
       "18327    <html>\\n<head>\\n<meta http-equiv=\"Content-Type...\n",
       "18930    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "18931    <html><head>\\n<meta http-equiv=\"Content-Type\" ...\n",
       "18934    <html><head>\\n<meta http-equiv=\"Content-Type\" ...\n",
       "18935    <html xmlns:v=\"urn:schemas-microsoft-com:vml\" ...\n",
       "18936    <html><head>\\n<meta http-equiv=\"Content-Type\" ...\n",
       "18937    <html><head>\\n<meta http-equiv=\"Content-Type\" ...\n",
       "18938    <html><head>\\n<meta http-equiv=\"Content-Type\" ...\n",
       "18939    <html><head>\\n<meta http-equiv=\"Content-Type\" ...\n",
       "18942    <html><head>\\n<meta http-equiv=\"Content-Type\" ...\n",
       "Name: body_html, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"body_html\"][df[\"body_html\"].str.find(\"ordre du jour de l\") != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0b23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        <DB8PR08MB505240FF56402FB9D696487BC2830@DB8PR0...\n",
       "1                                                         \n",
       "2             <5ecc35d6.1c69fb81.1333f.f055@mx.google.com>\n",
       "3             <5ecc35d6.1c69fb81.1333f.f055@mx.google.com>\n",
       "4        <1241336830.665580.1590440188542.JavaMail.zimb...\n",
       "                               ...                        \n",
       "19131                                                     \n",
       "19132                                                     \n",
       "19133                                                     \n",
       "19134    <f722ad124b5c4274a6345dfcd950fe8a@archivistes....\n",
       "19135                                                     \n",
       "Name: in_reply_to, Length: 19136, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"in_reply_to\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The body_clean_text column already exists\n",
      "Processing 18793 of 18793 HTML emails...\n",
      "Processing batch 1 (1-500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 138.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2 (501-1000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 142.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 3 (1001-1500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 141.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 4 (1501-2000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 188.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 5 (2001-2500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 169.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 6 (2501-3000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 156.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 7 (3001-3500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 208.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 8 (3501-4000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 199.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 9 (4001-4500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 167.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 10 (4501-5000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 153.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 11 (5001-5500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 172.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 12 (5501-6000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 156.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 13 (6001-6500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 188.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 14 (6501-7000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 191.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 15 (7001-7500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 218.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 16 (7501-8000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 185.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 17 (8001-8500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 197.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 18 (8501-9000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 158.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 19 (9001-9500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 163.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 20 (9501-10000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 140.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 21 (10001-10500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 161.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 22 (10501-11000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 158.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 23 (11001-11500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 170.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 24 (11501-12000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 217.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 25 (12001-12500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 209.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 26 (12501-13000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 186.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 27 (13001-13500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 220.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 28 (13501-14000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 216.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 29 (14001-14500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 164.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 30 (14501-15000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 214.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 31 (15001-15500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:02<00:00, 214.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 32 (15501-16000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 130.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 33 (16001-16500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 140.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 34 (16501-17000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 154.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 35 (17001-17500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 142.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 36 (17501-18000 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 165.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 37 (18001-18500 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [00:03<00:00, 149.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 38 (18501-18793 of 18793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 293/293 [00:02<00:00, 102.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Total processed: 18793 HTML emails + 0 text emails\n"
     ]
    }
   ],
   "source": [
    "def extract_clean_text_from_html(html_content):\n",
    "    \"\"\"\n",
    "    Extract clean, readable text from HTML content.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): HTML content to clean\n",
    "\n",
    "    Returns:\n",
    "        str: Clean text without HTML tags\n",
    "    \"\"\"\n",
    "    if not html_content:\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Remove scripts, styles, and other tags that contain content we don't want\n",
    "        html_content = re.sub(r'<(script|style|head).*?>.*?</\\1>', ' ', html_content, flags=re.DOTALL)\n",
    "\n",
    "        # Replace common block elements with newlines to preserve structure\n",
    "        html_content = re.sub(r'</(p|div|h\\d|tr|li)>', '\\n', html_content)\n",
    "        html_content = re.sub(r'<br[^>]*>', '\\n', html_content)\n",
    "\n",
    "        # Replace table cells with tab separation\n",
    "        html_content = re.sub(r'</td>', '\\t', html_content)\n",
    "\n",
    "        # Remove all HTML tags\n",
    "        text = re.sub(r'<[^>]+>', ' ', html_content)\n",
    "\n",
    "        # Decode HTML entities (&nbsp;, &lt;, etc.)\n",
    "        text = html.unescape(text)\n",
    "\n",
    "        # Handle literal escape sequences that appear in the text\n",
    "        # Replace literal \"\\xad\" with empty string (remove soft hyphens)\n",
    "        text = text.replace('\\\\xad', '')\n",
    "        # Replace literal \"\\xa0\" with a space (non-breaking spaces)\n",
    "        text = text.replace('\\\\xa0', ' ')\n",
    "\n",
    "        # Handle actual Unicode characters too\n",
    "        # Remove soft hyphens (invisible hyphens used for word breaks)\n",
    "        text = text.replace('\\xad', '')\n",
    "        # Replace non-breaking spaces with regular spaces\n",
    "        text = text.replace('\\xa0', ' ')\n",
    "        # Remove other problematic control characters\n",
    "        text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f]', '', text)\n",
    "\n",
    "        # Clean up other escape sequences that might appear in text\n",
    "        text = text.replace('\\\\\\\\', '\\\\')  # Double backslash to single\n",
    "        text = text.replace(\"\\\\'\", \"'\")    # Escaped single quote\n",
    "        text = text.replace('\\\\\"', '\"')    # Escaped double quote\n",
    "        text = text.replace('\\\\n', '\\n')   # Literal \\n to newline\n",
    "        text = text.replace('\\\\t', '\\t')   # Literal \\t to tab\n",
    "\n",
    "        # Remove remaining literal escape sequences like \\x.. that weren't handled above\n",
    "        text = re.sub(r'\\\\x[0-9a-fA-F]{2}', '', text)\n",
    "\n",
    "        # Clean up whitespace (multiple spaces, tabs, newlines)\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "        # Final cleanup to remove leading/trailing whitespace\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing HTML: {e}\")\n",
    "        return f\"Error processing HTML content: {str(e)}\"\n",
    "\n",
    "\n",
    "def process_database(db_path, limit=None, batch_size=500, verbose=True):\n",
    "    \"\"\"Process the entire database, adding clean text for all HTML emails\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Add column if it doesn't exist\n",
    "    try:\n",
    "        cursor.execute(\"ALTER TABLE emails ADD COLUMN body_clean_text TEXT\")\n",
    "        print(\"Added body_clean_text column to database\")\n",
    "    except sqlite3.OperationalError:\n",
    "        if verbose:\n",
    "            print(\"The body_clean_text column already exists\")\n",
    "\n",
    "    # Count HTML emails\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM emails WHERE has_html = 1\")\n",
    "    total_html = cursor.fetchone()[0]\n",
    "\n",
    "    if limit:\n",
    "        total_to_process = min(limit, total_html)\n",
    "    else:\n",
    "        total_to_process = total_html\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Processing {total_to_process} of {total_html} HTML emails...\")\n",
    "\n",
    "    # Process HTML emails in batches\n",
    "    processed = 0\n",
    "    for offset in range(0, total_to_process, batch_size):\n",
    "        if limit and offset >= limit:\n",
    "            break\n",
    "\n",
    "        current_batch = min(batch_size, total_to_process - offset)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Processing batch {offset//batch_size + 1} ({offset+1}-{offset+current_batch} of {total_to_process})\")\n",
    "\n",
    "        # Get batch of emails\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT rowid, message_id, body_html\n",
    "            FROM emails\n",
    "            WHERE has_html = 1\n",
    "            LIMIT {current_batch} OFFSET {offset}\n",
    "        \"\"\")\n",
    "\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        # Process each email in the batch\n",
    "        for row in tqdm(rows, desc=\"Processing\", disable=not verbose):\n",
    "            rowid, message_id, body_html = row\n",
    "\n",
    "            if body_html:\n",
    "                clean_text = extract_clean_text_from_html(body_html)\n",
    "\n",
    "                # Update database\n",
    "                cursor.execute(\n",
    "                    \"UPDATE emails SET body_clean_text = ? WHERE rowid = ?\",\n",
    "                    (clean_text, rowid)\n",
    "                )\n",
    "\n",
    "                processed += 1\n",
    "\n",
    "        # Commit after each batch\n",
    "        conn.commit()\n",
    "\n",
    "    # Process non-HTML emails (copy body_text to body_clean_text)\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM emails WHERE has_html = 0 AND body_clean_text IS NULL\")\n",
    "    non_html_count = cursor.fetchone()[0]\n",
    "\n",
    "    if non_html_count > 0:\n",
    "        if verbose:\n",
    "            print(f\"Copying text for {non_html_count} non-HTML emails...\")\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            UPDATE emails\n",
    "            SET body_clean_text = body_text\n",
    "            WHERE has_html = 0 AND body_clean_text IS NULL\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Processing complete. Total processed: {processed} HTML emails + {non_html_count} text emails\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "class EmailAnalyzer:\n",
    "    \"\"\"Class for analyzing the email database\"\"\"\n",
    "\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Connect to the database\"\"\"\n",
    "        if not self.conn:\n",
    "            self.conn = sqlite3.connect(self.db_path)\n",
    "            # Configure SQLite to return datetime objects for timestamps\n",
    "            self.conn.row_factory = sqlite3.Row\n",
    "        return self.conn\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the database connection\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            self.conn = None\n",
    "\n",
    "    def get_email_summary(self):\n",
    "        \"\"\"Get a summary of emails in the database\"\"\"\n",
    "        conn = self.connect()\n",
    "\n",
    "        # Get basic statistics\n",
    "        stats = {}\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Total emails\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM emails\")\n",
    "        stats['total_emails'] = cursor.fetchone()[0]\n",
    "\n",
    "        # Emails by folder\n",
    "        cursor.execute(\"SELECT folder, COUNT(*) as count FROM emails GROUP BY folder ORDER BY count DESC\")\n",
    "        stats['emails_by_folder'] = [dict(row) for row in cursor.fetchall()]\n",
    "\n",
    "        # Emails by year\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT strftime('%Y', date) AS year, COUNT(*) AS count\n",
    "            FROM emails\n",
    "            GROUP BY year\n",
    "            ORDER BY year\n",
    "        \"\"\")\n",
    "        stats['emails_by_year'] = [dict(row) for row in cursor.fetchall()]\n",
    "\n",
    "        # Top senders\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT \"from\", COUNT(*) AS count\n",
    "            FROM emails\n",
    "            GROUP BY \"from\"\n",
    "            ORDER BY count DESC\n",
    "            LIMIT 10\n",
    "        \"\"\")\n",
    "        stats['top_senders'] = [dict(row) for row in cursor.fetchall()]\n",
    "\n",
    "        # Emails with attachments\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM emails WHERE attachment_count > 0\")\n",
    "        stats['emails_with_attachments'] = cursor.fetchone()[0]\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def search_emails(self, query, limit=100):\n",
    "        \"\"\"Search emails by text content\"\"\"\n",
    "        conn = self.connect()\n",
    "\n",
    "        # Check if body_clean_text exists\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"PRAGMA table_info(emails)\")\n",
    "        columns = [row[1] for row in cursor.fetchall()]\n",
    "\n",
    "        if 'body_clean_text' in columns:\n",
    "            search_column = 'body_clean_text'\n",
    "        else:\n",
    "            # Fall back to body_text if clean text doesn't exist\n",
    "            search_column = 'body_text'\n",
    "\n",
    "        # Perform the search\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT message_id, subject, \"from\", \"to\", date, folder\n",
    "            FROM emails\n",
    "            WHERE {search_column} LIKE ?\n",
    "            ORDER BY date DESC\n",
    "            LIMIT ?\n",
    "        \"\"\", (f'%{query}%', limit))\n",
    "\n",
    "        return [dict(row) for row in cursor.fetchall()]\n",
    "\n",
    "    def get_email_content(self, message_id):\n",
    "        \"\"\"Get full content of a specific email by message_id\"\"\"\n",
    "        conn = self.connect()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Check if body_clean_text exists\n",
    "        cursor.execute(\"PRAGMA table_info(emails)\")\n",
    "        columns = [row[1] for row in cursor.fetchall()]\n",
    "\n",
    "        if 'body_clean_text' in columns:\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT *, body_clean_text AS content\n",
    "                FROM emails\n",
    "                WHERE message_id = ?\n",
    "            \"\"\", (message_id,))\n",
    "        else:\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT *,\n",
    "                    CASE WHEN has_html = 1 THEN body_html ELSE body_text END AS content\n",
    "                FROM emails\n",
    "                WHERE message_id = ?\n",
    "            \"\"\", (message_id,))\n",
    "\n",
    "        result = cursor.fetchone()\n",
    "        if result:\n",
    "            return dict(result)\n",
    "        return None\n",
    "\n",
    "    def get_conversation_thread(self, message_id):\n",
    "        \"\"\"Get all emails in the same conversation thread\"\"\"\n",
    "        conn = self.connect()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # First get the current email to find its references or in-reply-to\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT message_id, \"references\", in_reply_to, subject\n",
    "            FROM emails\n",
    "            WHERE message_id = ?\n",
    "        \"\"\", (message_id,))\n",
    "\n",
    "        email = cursor.fetchone()\n",
    "        if not email:\n",
    "            return []\n",
    "\n",
    "        # Find related messages by references, in-reply-to, or subject thread\n",
    "        message_ids = set()\n",
    "\n",
    "        # Add current message\n",
    "        message_ids.add(email['message_id'])\n",
    "\n",
    "        # Add messages this email is replying to\n",
    "        if email['in_reply_to']:\n",
    "            message_ids.add(email['in_reply_to'])\n",
    "\n",
    "        # Add messages referenced\n",
    "        if email['references']:\n",
    "            ref_ids = re.findall(r'<([^>]+)>', email['references'])\n",
    "            message_ids.update(ref_ids)\n",
    "\n",
    "        # Find messages that reply to this one\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT message_id\n",
    "            FROM emails\n",
    "            WHERE in_reply_to = ?\n",
    "        \"\"\", (email['message_id'],))\n",
    "\n",
    "        for row in cursor.fetchall():\n",
    "            message_ids.add(row['message_id'])\n",
    "\n",
    "        # Also find messages with the same subject (ignoring Re:, Fwd:, etc.)\n",
    "        if email['subject']:\n",
    "            clean_subject = re.sub(r'^(Re|Fwd|Fw|TR)(\\[\\d+\\])?:\\s*', '', email['subject'], flags=re.IGNORECASE)\n",
    "            if clean_subject:\n",
    "                cursor.execute(\"\"\"\n",
    "                    SELECT message_id\n",
    "                    FROM emails\n",
    "                    WHERE subject LIKE ? AND message_id != ?\n",
    "                \"\"\", (f'%{clean_subject}%', email['message_id']))\n",
    "\n",
    "                for row in cursor.fetchall():\n",
    "                    message_ids.add(row['message_id'])\n",
    "\n",
    "        # Now get all the emails in the thread\n",
    "        placeholders = ','.join(['?'] * len(message_ids))\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT message_id, subject, \"from\", date, \"to\", cc\n",
    "            FROM emails\n",
    "            WHERE message_id IN ({placeholders})\n",
    "            ORDER BY date\n",
    "        \"\"\", list(message_ids))\n",
    "\n",
    "        return [dict(row) for row in cursor.fetchall()]\n",
    "\n",
    "\n",
    "    def export_to_dataframe(self, query=None, limit=None):\n",
    "        \"\"\"Export emails to a pandas DataFrame for analysis\"\"\"\n",
    "        conn = self.connect()\n",
    "\n",
    "        if query:\n",
    "            sql = f\"{query} \"\n",
    "            if limit:\n",
    "                sql += f\"LIMIT {limit}\"\n",
    "            df = pd.read_sql_query(sql, conn)\n",
    "        else:\n",
    "            # Default query to get important fields\n",
    "            sql = \"\"\"\n",
    "                SELECT message_id, subject, \"from\", \"to\", date, folder,\n",
    "                       attachment_count, body_clean_text\n",
    "                FROM emails\n",
    "            \"\"\"\n",
    "            if limit:\n",
    "                sql += f\" LIMIT {limit}\"\n",
    "            df = pd.read_sql_query(sql, conn)\n",
    "\n",
    "        # Convert date strings to datetime objects\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "        return df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the script from the command line\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Process emails in SQLite database to extract clean text')\n",
    "    parser.add_argument('db_path', help='Path to the SQLite database')\n",
    "    parser.add_argument('--limit', type=int, help='Limit the number of emails to process')\n",
    "    parser.add_argument('--batch-size', type=int, default=500, help='Batch size for processing')\n",
    "    parser.add_argument('--summary', action='store_true', help='Print a summary of the database')\n",
    "    parser.add_argument('--search', help='Search for emails containing a specific text')\n",
    "    parser.add_argument('--quiet', action='store_true', help='Reduce output verbosity')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.summary:\n",
    "        analyzer = EmailAnalyzer(args.db_path)\n",
    "        summary = analyzer.get_email_summary()\n",
    "\n",
    "        print(f\"Email Database Summary\")\n",
    "        print(f\"====================\")\n",
    "        print(f\"Total emails: {summary['total_emails']}\")\n",
    "        print(f\"Emails with attachments: {summary['emails_with_attachments']}\")\n",
    "\n",
    "        print(\"\\nEmails by folder:\")\n",
    "        for folder in summary['emails_by_folder'][:10]:  # Show top 10\n",
    "            print(f\"  {folder['folder']}: {folder['count']}\")\n",
    "\n",
    "        print(\"\\nEmails by year:\")\n",
    "        for year in summary['emails_by_year']:\n",
    "            print(f\"  {year['year']}: {year['count']}\")\n",
    "\n",
    "        print(\"\\nTop senders:\")\n",
    "        for sender in summary['top_senders'][:5]:  # Show top 5\n",
    "            print(f\"  {sender['from']}: {sender['count']}\")\n",
    "\n",
    "        analyzer.close()\n",
    "    elif args.search:\n",
    "        analyzer = EmailAnalyzer(args.db_path)\n",
    "        results = analyzer.search_emails(args.search)\n",
    "\n",
    "        print(f\"Search results for '{args.search}':\")\n",
    "        print(f\"====================\")\n",
    "        for i, email in enumerate(results):\n",
    "            date = email['date']\n",
    "            if date:\n",
    "                try:\n",
    "                    date = datetime.datetime.fromisoformat(date)\n",
    "                    date_str = date.strftime('%Y-%m-%d %H:%M')\n",
    "                except (ValueError, TypeError):\n",
    "                    date_str = str(date)\n",
    "            else:\n",
    "                date_str = 'Unknown'\n",
    "\n",
    "            print(f\"{i+1}. [{date_str}] {email['subject']}\")\n",
    "            print(f\"   From: {email['from']}\")\n",
    "            print(f\"   To: {email['to']}\")\n",
    "            print(f\"   ID: {email['message_id']}\")\n",
    "            print()\n",
    "\n",
    "        analyzer.close()\n",
    "    else:\n",
    "        # Process the database\n",
    "        process_database(args.db_path, limit=args.limit, batch_size=args.batch_size, verbose=not args.quiet)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "# main()\n",
    "db_path = \"emails.db\"  # Update this path if needed\n",
    "process_database(db_path, limit=None, batch_size=500, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37603874",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EmailAnalyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m analyzer =\u001b[43mEmailAnalyzer\u001b[49m(db_path)\n\u001b[32m      2\u001b[39m analyzer.get_email_summary()\n",
      "\u001b[31mNameError\u001b[39m: name 'EmailAnalyzer' is not defined"
     ]
    }
   ],
   "source": [
    "analyzer =EmailAnalyzer(db_path)\n",
    "analyzer.get_email_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4cadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'message_id': '<0e8d36fce37f4240b8b7f0965b794923@archivistes.org>',\n",
       "  'subject': \"TR: Rappel : Assemblée générale de l'AAF le 29 juin 2020 à partir de 17h en visioconférence\",\n",
       "  'from': 'AAF - Anne Clerc, déléguée générale<delegation_generale@archivistes.org>',\n",
       "  'date': '2020-06-10 16:22:01+02:00',\n",
       "  'to': 'AAF - Céline Guyon <celine.guyon@archivistes.org>',\n",
       "  'cc': 'Catherine Bernard <catherine.bernard.aaf@gmail.com>, \"Ducol, Laurent\"\\n\\t<Laurent.Ducol@saint-gobain.com>, AAF vie associative - Nicolas Didon\\n\\t<vieassociative@archivistes.org>'}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.get_conversation_thread(message_id=\"<0e8d36fce37f4240b8b7f0965b794923@archivistes.org>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b83b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122649/1979878644.py:340: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df['date'] = pd.to_datetime(df['date'], errors='coerce')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Aux membres de l’AAF \\n\\n \\n\\n \\n \\n \\n \\n Cher.e membre,\\n\\nJ’ai l’honneur de vous informer que l’assemblée générale ordinaire annuelle de notre association se tiendra le :\\n\\n Lundi 29 juin 2020 de 17h à 19h en visioconférence\\n \\n\\nInitialement prévue le 20 mars, l’assemblée générale a été reportée en raison du contexte sanitaire.\\n\\nL’ordre du jour de l’assemblée générale est le suivant :\\n\\n Approbation du procès-verbal de la précédente réunion d’avril 2019 ;\\n\\n Approbation du rapport moral de la présidente pour l’année 2019 ;\\n\\n Approbation du rapport financier de la trésorière, approbation des comptes\\n de l’exercice clos le 31 décembre 2019 ;\\n\\n Présentation des comptes de l’exercice clos le 31 décembre 2019 pour la filiale\\n Archivistes Français Formation ;\\n\\n Présentation et approbation du budget 2020 révisé pour l’AAF ;\\n\\n Présentation du budget 2020 révisé pour la filiale Archivistes Français Formation ;\\n\\n Fixation du montant des cotisations pour l’année 2021.\\n\\nJe vous invite à prendre connaissance de l’ensemble des documents et informations relatifs à l’activité et à la situation financière de l’AAF et de sa filiale Archivistes Français Formation (rapport moral de la présidente, rapport financier et\\n comptes de résultat, budgets prévisionnels, rapports des sections et des comités, montant des cotisations pour 2021), en ligne, sur le site Internet de l’association : [ https://www.archivistes.org/-AG-du-2020_03_20 - ].\\n Il faut vous connecter avec vos identifiants pour y avoir accès. \\n\\n Pour participer à l’AG en visioconférence, merci de vous inscrire préalablement ici : \\n\\n https://www.weezevent.com/participation-a-l-ag-2020 \\n \\n\\nLes modalités de connexion et d’accès à l’AG en ligne vous seront transmises ultérieurement. \\n\\n \\n\\n Votes : \\n\\nLes votes se feront en ligne à compter du 29 juin, 17h, jusqu’au 1er juillet, 17h, via un formulaire qui vous sera communiqué à l’ouverture de l’AG. Si vous ne pouvez pas assister à cet évènement en direct, vous pourrez néanmoins accéder aux vidéos\\n de présentation qui seront accessibles sur notre site, à compter du 30 juin au matin, vous permettant de voter dans les temps impartis. \\n\\n \\n\\n Seuls les adhérents individuels et les mandataires des organismes adhérents à jour de cotisation pourront voter et recevront le lien vers le formulaire. \\n\\n \\n\\n Les adhérents individuels doivent être à jour de leurs cotisations 2019 et 2020 (compte tenu cette année de la date de l’assemblée générale). \\n\\n Si ce n’est déjà fait, les adhérents individuels sont invités à régler en ligne sur leur Extranet leurs cotisations.\\n \\n\\n Vous avez reçu un mel avec vos identifiants le 28 janvier dernier. Si vous ne le retrouvez pas, n’hésitez pas à contacter Nicolas Didon, en charge de la vie associative ( vieassociative@archivistes.org ). \\n\\n \\n\\nComptant sur votre participation, je vous prie d’agréer, cher.e membre, l’expression de mes salutations amicales.\\n\\n \\n\\n \\n \\n \\n \\n Céline GUYON \\n\\n Présidente de l’Association des archivistes français \\n\\n8 rue Jean-Marie Jégo - 75013 Paris\\n\\nStandard : 01 46 06 39 44\\n\\n presidence@archivistes.org \\n\\n www.archivistes.org'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_db_cleaned = analyzer.export_to_dataframe()\n",
    "df_db_cleaned[\"body_clean_text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a458d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## attempt 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf015a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mailbox\n",
    "import pandas as pd\n",
    "import email\n",
    "import os\n",
    "from email.header import decode_header\n",
    "import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import html\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "def decode_str(s):\n",
    "    \"\"\"Decode encoded email header strings\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        decoded_parts = decode_header(s)\n",
    "        return ''.join([\n",
    "            part.decode(encoding or 'utf-8', errors='replace') if isinstance(part, bytes) else part\n",
    "            for part, encoding in decoded_parts\n",
    "        ])\n",
    "    except:\n",
    "        return str(s)\n",
    "\n",
    "def extract_clean_text_from_html(html_content):\n",
    "    \"\"\"\n",
    "    Extract clean, readable text from HTML content.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): HTML content to clean\n",
    "\n",
    "    Returns:\n",
    "        str: Clean text without HTML tags\n",
    "    \"\"\"\n",
    "    if not html_content:\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Remove scripts, styles, and other tags that contain content we don't want\n",
    "        html_content = re.sub(r'<(script|style|head).*?>.*?</\\1>', ' ', html_content, flags=re.DOTALL)\n",
    "\n",
    "        # Replace common block elements with newlines to preserve structure\n",
    "        html_content = re.sub(r'</(p|div|h\\d|tr|li)>', '\\n', html_content)\n",
    "        html_content = re.sub(r'<br[^>]*>', '\\n', html_content)\n",
    "\n",
    "        # Replace table cells with tab separation\n",
    "        html_content = re.sub(r'</td>', '\\t', html_content)\n",
    "\n",
    "        # Remove all HTML tags\n",
    "        text = re.sub(r'<[^>]+>', ' ', html_content)\n",
    "\n",
    "        # Decode HTML entities (&nbsp;, &lt;, etc.)\n",
    "        text = html.unescape(text)\n",
    "\n",
    "        # Handle literal escape sequences that appear in the text\n",
    "        # Replace literal \"\\xad\" with empty string (remove soft hyphens)\n",
    "        text = text.replace('\\\\xad', '')\n",
    "        # Replace literal \"\\xa0\" with a space (non-breaking spaces)\n",
    "        text = text.replace('\\\\xa0', ' ')\n",
    "\n",
    "        # Handle actual Unicode characters too\n",
    "        # Remove soft hyphens (invisible hyphens used for word breaks)\n",
    "        text = text.replace('\\xad', '')\n",
    "        # Replace non-breaking spaces with regular spaces\n",
    "        text = text.replace('\\xa0', ' ')\n",
    "        # Remove other problematic control characters\n",
    "        text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f]', '', text)\n",
    "\n",
    "        # Clean up other escape sequences that might appear in text\n",
    "        text = text.replace('\\\\\\\\', '\\\\')  # Double backslash to single\n",
    "        text = text.replace(\"\\\\'\", \"'\")    # Escaped single quote\n",
    "        text = text.replace('\\\\\"', '\"')    # Escaped double quote\n",
    "        text = text.replace('\\\\n', '\\n')   # Literal \\n to newline\n",
    "        text = text.replace('\\\\t', '\\t')   # Literal \\t to tab\n",
    "\n",
    "        # Remove remaining literal escape sequences like \\x.. that weren't handled above\n",
    "        text = re.sub(r'\\\\x[0-9a-fA-F]{2}', '', text)\n",
    "\n",
    "        # Clean up whitespace (multiple spaces, tabs, newlines)\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "        # Final cleanup to remove leading/trailing whitespace\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing HTML: {e}\")\n",
    "        return f\"Error processing HTML content: {str(e)}\"\n",
    "\n",
    "def get_email_body(message):\n",
    "    \"\"\"Extract body text from email message, handling HTML correctly\"\"\"\n",
    "    body_text = \"\"\n",
    "    body_html = \"\"\n",
    "\n",
    "    if message.is_multipart():\n",
    "        for part in message.walk():\n",
    "            content_type = part.get_content_type()\n",
    "            content_disposition = str(part.get(\"Content-Disposition\") or \"\")\n",
    "\n",
    "            # Skip attachments\n",
    "            if \"attachment\" in content_disposition:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                payload = part.get_payload(decode=True)\n",
    "                if payload is None:\n",
    "                    continue\n",
    "\n",
    "                charset = part.get_content_charset() or 'utf-8'\n",
    "                decoded_payload = payload.decode(charset, errors='replace')\n",
    "\n",
    "                if content_type == \"text/plain\":\n",
    "                    body_text += decoded_payload\n",
    "                elif content_type == \"text/html\":\n",
    "                    body_html += decoded_payload\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        # Not multipart - get payload directly\n",
    "        try:\n",
    "            content_type = message.get_content_type()\n",
    "            payload = message.get_payload(decode=True)\n",
    "            if payload:\n",
    "                charset = message.get_content_charset() or 'utf-8'\n",
    "                decoded_payload = payload.decode(charset, errors='replace')\n",
    "\n",
    "                if content_type == \"text/plain\":\n",
    "                    body_text = decoded_payload\n",
    "                elif content_type == \"text/html\":\n",
    "                    body_html = decoded_payload\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Prefer HTML content but fall back to plain text\n",
    "    if body_html:\n",
    "        return {\n",
    "            \"html\": body_html,\n",
    "            \"text\": extract_text_from_html(body_html),\n",
    "            \"has_html\": True\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"html\": \"\",\n",
    "            \"text\": body_text,\n",
    "            \"has_html\": False\n",
    "        }\n",
    "\n",
    "def extract_attachments_info(message):\n",
    "    \"\"\"Extract information about attachments\"\"\"\n",
    "    attachments = []\n",
    "\n",
    "    if message.is_multipart():\n",
    "        for part in message.walk():\n",
    "            content_disposition = str(part.get(\"Content-Disposition\") or \"\")\n",
    "\n",
    "            if \"attachment\" in content_disposition:\n",
    "                filename = part.get_filename()\n",
    "                if filename:\n",
    "                    try:\n",
    "                        filename = decode_str(filename)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                content_type = part.get_content_type()\n",
    "                attachments.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"content_type\": content_type,\n",
    "                    \"size\": len(part.get_payload(decode=True) or b'')\n",
    "                })\n",
    "\n",
    "    return attachments\n",
    "\n",
    "def extract_recipients(message):\n",
    "    \"\"\"Extract all recipients (To, CC, BCC)\"\"\"\n",
    "    to = decode_str(message.get('to') or \"\")\n",
    "    cc = decode_str(message.get('cc') or \"\")\n",
    "    bcc = decode_str(message.get('bcc') or \"\")\n",
    "\n",
    "    return {\n",
    "        \"to\": to,\n",
    "        \"cc\": cc,\n",
    "        \"bcc\": bcc\n",
    "    }\n",
    "\n",
    "def extract_message_data(message, folder_name):\n",
    "    \"\"\"Extract comprehensive email data\"\"\"\n",
    "    # Extract basic headers\n",
    "    subject = decode_str(message.get('subject') or \"\")\n",
    "    from_addr = decode_str(message.get('from') or \"\")\n",
    "    date_str = message.get('date')\n",
    "    message_id = decode_str(message.get('message-id') or \"\")\n",
    "\n",
    "    # Parse date\n",
    "    try:\n",
    "        date = email.utils.parsedate_to_datetime(date_str)\n",
    "    except:\n",
    "        date = None\n",
    "\n",
    "    # Get recipients\n",
    "    recipients = extract_recipients(message)\n",
    "\n",
    "    # Get body content\n",
    "    body_content = get_email_body(message)\n",
    "\n",
    "    # Get attachment info\n",
    "    attachments = extract_attachments_info(message)\n",
    "\n",
    "    # Thread information\n",
    "    references = decode_str(message.get('references') or \"\")\n",
    "    in_reply_to = decode_str(message.get('in-reply-to') or \"\")\n",
    "\n",
    "    return {\n",
    "        'message_id': message_id,\n",
    "        'subject': subject,\n",
    "        'from': from_addr,\n",
    "        'to': recipients[\"to\"],\n",
    "        'cc': recipients[\"cc\"],\n",
    "        'bcc': recipients[\"bcc\"],\n",
    "        'date': date,\n",
    "        'folder': folder_name,\n",
    "        'body_text': body_content[\"text\"],\n",
    "        'body_html': body_content[\"html\"],\n",
    "        'has_html': body_content[\"has_html\"],\n",
    "        'attachments': attachments,\n",
    "        'attachment_count': len(attachments),\n",
    "        'references': references,\n",
    "        'in_reply_to': in_reply_to\n",
    "    }\n",
    "\n",
    "def process_mbox_to_sqlite(mbox_path, conn, batch_size=100):\n",
    "    \"\"\"Process mbox file directly to SQLite in batches with progress bar\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    mbox = mailbox.mbox(mbox_path)\n",
    "    folder_name = os.path.basename(mbox_path).replace('.mbox', '')\n",
    "\n",
    "    # Count messages for progress bar\n",
    "    total_messages = len(mbox)\n",
    "    print(f\"Processing {folder_name} ({total_messages} messages)\")\n",
    "\n",
    "    # Process in batches\n",
    "    batch = []\n",
    "    for i, message in enumerate(tqdm(mbox, total=total_messages, desc=folder_name)):\n",
    "        email_data = extract_message_data(message, folder_name)\n",
    "\n",
    "        # Convert attachment data to string\n",
    "        if email_data['attachments']:\n",
    "            email_data['attachments_json'] = str(email_data['attachments'])\n",
    "        else:\n",
    "            email_data['attachments_json'] = \"\"\n",
    "\n",
    "        del email_data['attachments']  # Remove original list\n",
    "\n",
    "        batch.append(email_data)\n",
    "\n",
    "        # Process batch\n",
    "        if len(batch) >= batch_size or i == total_messages - 1:\n",
    "            if batch:\n",
    "                # Convert to DataFrame for easy SQL insertion\n",
    "                df = pd.DataFrame(batch)\n",
    "\n",
    "                # Write to database\n",
    "                df.to_sql('emails', conn, if_exists='append', index=False)\n",
    "\n",
    "                # Clear batch\n",
    "                batch = []\n",
    "\n",
    "                # Commit to save progress\n",
    "                conn.commit()\n",
    "\n",
    "def setup_database(db_path):\n",
    "    \"\"\"Set up the SQLite database schema with proper types and indexes\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Drop table if exists\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS emails\")\n",
    "\n",
    "    # Create table with proper columns and types\n",
    "    # Note: Escaping the reserved keyword 'references' with quotes\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE emails (\n",
    "        message_id TEXT,\n",
    "        subject TEXT,\n",
    "        \"from\" TEXT,\n",
    "        \"to\" TEXT,\n",
    "        cc TEXT,\n",
    "        bcc TEXT,\n",
    "        date TIMESTAMP,\n",
    "        folder TEXT,\n",
    "        body_text TEXT,\n",
    "        body_html TEXT,\n",
    "        has_html BOOLEAN,\n",
    "        attachments_json TEXT,\n",
    "        attachment_count INTEGER,\n",
    "        \"references\" TEXT,\n",
    "        in_reply_to TEXT\n",
    "    )\n",
    "    ''')\n",
    "\n",
    "    # Create indexes for common queries\n",
    "    cursor.execute('CREATE INDEX idx_date ON emails(date)')\n",
    "    cursor.execute('CREATE INDEX idx_folder ON emails(folder)')\n",
    "    cursor.execute('CREATE INDEX idx_from ON emails(\"from\")')\n",
    "    cursor.execute('CREATE INDEX idx_to ON emails(\"to\")')\n",
    "    cursor.execute('CREATE INDEX idx_subject ON emails(subject)')\n",
    "\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def collect_email_data(directory, include_html=True, include_attachments=True):\n",
    "    \"\"\"Process all mbox files and return a list of email data\"\"\"\n",
    "    all_emails = []\n",
    "\n",
    "    # Get list of mbox files\n",
    "    mbox_files = [f for f in os.listdir(directory) if f.endswith('.mbox')]\n",
    "    print(f\"Found {len(mbox_files)} mbox files\")\n",
    "\n",
    "    # Process each file\n",
    "    for filename in mbox_files:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        folder_name = os.path.basename(file_path).replace('.mbox', '')\n",
    "        mbox = mailbox.mbox(file_path)\n",
    "\n",
    "        # Count messages for progress bar\n",
    "        total_messages = len(mbox)\n",
    "        print(f\"Processing {folder_name} ({total_messages} messages)\")\n",
    "\n",
    "        for message in tqdm(mbox, total=total_messages, desc=folder_name):\n",
    "            email_data = extract_message_data(message, folder_name)\n",
    "\n",
    "            # Optionally exclude HTML content to reduce data size\n",
    "            if not include_html:\n",
    "                email_data.pop('body_html', None)\n",
    "\n",
    "            # Optionally simplify attachment info to reduce data size\n",
    "            if not include_attachments:\n",
    "                email_data['attachment_count'] = len(email_data.get('attachments', []))\n",
    "                email_data.pop('attachments', None)\n",
    "\n",
    "            all_emails.append(email_data)\n",
    "\n",
    "    return all_emails\n",
    "\n",
    "def save_to_json(emails, filepath, indent=None):\n",
    "    \"\"\"Save email data to a JSON file\"\"\"\n",
    "    # Convert data for JSON serialization\n",
    "    serializable_emails = []\n",
    "\n",
    "    for email in emails:\n",
    "        email_copy = email.copy()\n",
    "\n",
    "        # Convert datetime to string\n",
    "        if email_copy.get('date') and isinstance(email_copy['date'], datetime.datetime):\n",
    "            email_copy['date'] = email_copy['date'].isoformat()\n",
    "\n",
    "        serializable_emails.append(email_copy)\n",
    "\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_emails, f, ensure_ascii=False, indent=indent)\n",
    "\n",
    "    print(f\"Saved {len(serializable_emails)} emails to {filepath}\")\n",
    "\n",
    "def save_to_csv(emails, filepath):\n",
    "    \"\"\"Save email data to a CSV file\"\"\"\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(emails)\n",
    "\n",
    "    # Convert datetime to string\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = df['date'].apply(lambda x: x.isoformat() if isinstance(x, datetime.datetime) else x)\n",
    "\n",
    "    # Convert attachments to string if they exist\n",
    "    if 'attachments' in df.columns:\n",
    "        df['attachments'] = df['attachments'].apply(lambda x: str(x) if x else \"\")\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"Saved {len(df)} emails to {filepath}\")\n",
    "\n",
    "def process_mbox_files(directory, output_format='sqlite', output_path=None, include_html=True, include_attachments=True):\n",
    "    \"\"\"\n",
    "    Process mbox files and save to the specified format\n",
    "\n",
    "    Parameters:\n",
    "    - directory: Directory containing .mbox files\n",
    "    - output_format: 'sqlite', 'json', 'csv', or 'all'\n",
    "    - output_path: Output file path (default: emails.<format>)\n",
    "    - include_html: Whether to include HTML content\n",
    "    - include_attachments: Whether to include attachment details\n",
    "    \"\"\"\n",
    "    if output_format not in ['sqlite', 'json', 'csv', 'all']:\n",
    "        raise ValueError(\"output_format must be one of 'sqlite', 'json', 'csv', or 'all'\")\n",
    "\n",
    "    # Set default output paths if not provided\n",
    "    if output_path is None:\n",
    "        output_path = 'emails'\n",
    "\n",
    "    # SQLite processing\n",
    "    if output_format in ['sqlite', 'all']:\n",
    "        sqlite_path = f\"{output_path}.db\" if not output_path.endswith('.db') else output_path\n",
    "        conn = setup_database(sqlite_path)\n",
    "\n",
    "        # Get list of mbox files\n",
    "        mbox_files = [f for f in os.listdir(directory) if f.endswith('.mbox')]\n",
    "\n",
    "        # Process each file\n",
    "        for filename in mbox_files:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            process_mbox_to_sqlite(file_path, conn)\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        print(f\"SQLite database saved to {sqlite_path}\")\n",
    "\n",
    "    # JSON and/or CSV processing\n",
    "    if output_format in ['json', 'csv', 'all']:\n",
    "        # Collect email data\n",
    "        emails = collect_email_data(directory, include_html, include_attachments)\n",
    "\n",
    "        # Save to JSON\n",
    "        if output_format in ['json', 'all']:\n",
    "            json_path = f\"{output_path}.json\" if not output_path.endswith('.json') else output_path\n",
    "            save_to_json(emails, json_path, indent=2)\n",
    "\n",
    "        # Save to CSV\n",
    "        if output_format in ['csv', 'all']:\n",
    "            csv_path = f\"{output_path}.csv\" if not output_path.endswith('.csv') else output_path\n",
    "            save_to_csv(emails, csv_path)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Process mbox files into various formats')\n",
    "    parser.add_argument('directory', help='Directory containing .mbox files')\n",
    "    parser.add_argument('--format', choices=['sqlite', 'json', 'csv', 'all'], default='sqlite',\n",
    "                       help='Output format (default: sqlite)')\n",
    "    parser.add_argument('--output', help='Output file path (without extension)')\n",
    "    parser.add_argument('--no-html', action='store_false', dest='include_html',\n",
    "                       help='Exclude HTML content to reduce file size')\n",
    "    parser.add_argument('--no-attachments', action='store_false', dest='include_attachments',\n",
    "                       help='Exclude attachment details to reduce file size')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    process_mbox_files(\n",
    "        args.directory,\n",
    "        args.format,\n",
    "        args.output,\n",
    "        args.include_html,\n",
    "        args.include_attachments\n",
    "    )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 mbox files\n",
      "Processing AG (6 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AG: 100%|██████████| 6/6 [00:00<00:00, 75.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Archive (10 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Archive: 100%|██████████| 10/10 [00:00<00:00, 19.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m mbox_directory = \u001b[33m\"\u001b[39m\u001b[33mdata/processed/mailbox_cecile/\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Update this path to your actual location\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Call the function to process the files and save as JSON\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mprocess_mbox_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmbox_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcsv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify 'json' as the output format\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43memails_export\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Optional: specify the output filename (without extension)\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_html\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Include HTML content (set to False to reduce file size)\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_attachments\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Include attachment details (set to False to reduce file size)\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 420\u001b[39m, in \u001b[36mprocess_mbox_files\u001b[39m\u001b[34m(directory, output_format, output_path, include_html, include_attachments)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# JSON and/or CSV processing\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_format \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mall\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    419\u001b[39m     \u001b[38;5;66;03m# Collect email data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     emails = \u001b[43mcollect_email_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_html\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_attachments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m     \u001b[38;5;66;03m# Save to JSON\u001b[39;00m\n\u001b[32m    423\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_format \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mall\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 326\u001b[39m, in \u001b[36mcollect_email_data\u001b[39m\u001b[34m(directory, include_html, include_attachments)\u001b[39m\n\u001b[32m    323\u001b[39m mbox = mailbox.mbox(file_path)\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# Count messages for progress bar\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m total_messages = \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_messages\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m messages)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m tqdm(mbox, total=total_messages, desc=folder_name):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/mailbox.py:637\u001b[39m, in \u001b[36m_singlefileMailbox.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    636\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a count of messages in the mailbox.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._toc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/mailbox.py:746\u001b[39m, in \u001b[36m_singlefileMailbox._lookup\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return (start, stop) or raise KeyError.\"\"\"\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._toc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_toc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/mailbox.py:867\u001b[39m, in \u001b[36mmbox._generate_toc\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    865\u001b[39m \u001b[38;5;28mself\u001b[39m._file.seek(\u001b[32m0\u001b[39m)\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m867\u001b[39m     line_pos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtell\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    868\u001b[39m     line = \u001b[38;5;28mself\u001b[39m._file.readline()\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m line.startswith(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFrom \u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Set the directory where your .mbox files are located\n",
    "mbox_directory = \"data/processed/mailbox_cecile/\"  # Update this path to your actual location\n",
    "\n",
    "# Call the function to process the files and save as JSON\n",
    "process_mbox_files(\n",
    "    directory=mbox_directory,\n",
    "    output_format='csv',  # Specify 'json' as the output format\n",
    "    output_path='emails_export',  # Optional: specify the output filename (without extension)\n",
    "    include_html=True,  # Include HTML content (set to False to reduce file size)\n",
    "    include_attachments=True  # Include attachment details (set to False to reduce file size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeffe05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 mbox files\n",
      "Processing AG (6 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AG: 100%|██████████| 6/6 [00:00<00:00, 32.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Archive (10 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Archive: 100%|██████████| 10/10 [00:00<00:00, 27.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Archives calssifiees (423 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Archives calssifiees: 100%|██████████| 423/423 [00:11<00:00, 37.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Ateliers (28 messages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ateliers: 100%|██████████| 28/28 [00:00<00:00, 109.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import the module\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Process to SQLite (default)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# process_mbox_files(\"path/to/mbox/files/\")\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Process to JSON\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mprocess_mbox_files\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/processed/mailbox_cecile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# # Process to CSV with a custom filename\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# process_mbox_files(\"path/to/mbox/files/\", output_format=\"csv\", output_path=\"my_emails\")\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# # Exclude HTML content for smaller files\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# process_mbox_files(\"path/to/mbox/files/\", output_format=\"json\", include_html=False)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 420\u001b[39m, in \u001b[36mprocess_mbox_files\u001b[39m\u001b[34m(directory, output_format, output_path, include_html, include_attachments)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# JSON and/or CSV processing\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_format \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mall\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    419\u001b[39m     \u001b[38;5;66;03m# Collect email data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     emails = \u001b[43mcollect_email_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_html\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_attachments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m     \u001b[38;5;66;03m# Save to JSON\u001b[39;00m\n\u001b[32m    423\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_format \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mall\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 326\u001b[39m, in \u001b[36mcollect_email_data\u001b[39m\u001b[34m(directory, include_html, include_attachments)\u001b[39m\n\u001b[32m    323\u001b[39m mbox = mailbox.mbox(file_path)\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# Count messages for progress bar\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m total_messages = \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_messages\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m messages)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m tqdm(mbox, total=total_messages, desc=folder_name):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/mailbox.py:637\u001b[39m, in \u001b[36m_singlefileMailbox.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    636\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a count of messages in the mailbox.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._toc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/mailbox.py:746\u001b[39m, in \u001b[36m_singlefileMailbox._lookup\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return (start, stop) or raise KeyError.\"\"\"\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._toc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_toc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/mailbox.py:868\u001b[39m, in \u001b[36mmbox._generate_toc\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    867\u001b[39m     line_pos = \u001b[38;5;28mself\u001b[39m._file.tell()\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m     line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m line.startswith(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFrom \u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    870\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stops) < \u001b[38;5;28mlen\u001b[39m(starts):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Import the module\n",
    "\n",
    "# Process to SQLite (default)\n",
    "# process_mbox_files(\"path/to/mbox/files/\")\n",
    "\n",
    "# Process to JSON\n",
    "process_mbox_files(mbox_directory, output_format=\"json\")\n",
    "\n",
    "# # Process to CSV with a custom filename\n",
    "# process_mbox_files(\"path/to/mbox/files/\", output_format=\"csv\", output_path=\"my_emails\")\n",
    "\n",
    "# # Generate all formats\n",
    "# process_mbox_files(\"path/to/mbox/files/\", output_format=\"all\")\n",
    "\n",
    "# # Exclude HTML content for smaller files\n",
    "# process_mbox_files(\"path/to/mbox/files/\", output_format=\"json\", include_html=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90771a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "from __future__ imports must occur at the beginning of the file (models.py, line 4)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92m~/.pyenv/versions/3.12.7/envs/olkoa_v3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3549\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[36m  \u001b[39m\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom models.models import EmailAddress, MailingList, Organisation, Position, Entity, Attachment, ReceiverEmail, SenderEmail\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/julie/Projects/olkoa/models/models.py:4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom __future__ import annotations\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m from __future__ imports must occur at the beginning of the file\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9ea01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attempt 5 with class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def decode_str(s):\n",
    "#     \"\"\"Decode encoded email header strings\"\"\"\n",
    "#     if s is None:\n",
    "#         return \"\"\n",
    "#     try:\n",
    "#         decoded_parts = decode_header(s)\n",
    "#         return ''.join([\n",
    "#             part.decode(encoding or 'utf-8', errors='replace') if isinstance(part, bytes) else part\n",
    "#             for part, encoding in decoded_parts\n",
    "#         ])\n",
    "#     except:\n",
    "#         return str(s)\n",
    "\n",
    "# def extract_clean_text_from_html(html_content):\n",
    "#     \"\"\"\n",
    "#     Extract clean, readable text from HTML content.\n",
    "\n",
    "#     Args:\n",
    "#         html_content (str): HTML content to clean\n",
    "\n",
    "#     Returns:\n",
    "#         str: Clean text without HTML tags\n",
    "#     \"\"\"\n",
    "#     if not html_content:\n",
    "#         return \"\"\n",
    "\n",
    "#     try:\n",
    "#         # Remove scripts, styles, and other tags that contain content we don't want\n",
    "#         html_content = re.sub(r'<(script|style|head).*?>.*?</\\1>', ' ', html_content, flags=re.DOTALL)\n",
    "\n",
    "#         # Replace common block elements with newlines to preserve structure\n",
    "#         html_content = re.sub(r'</(p|div|h\\d|tr|li)>', '\\n', html_content)\n",
    "#         html_content = re.sub(r'<br[^>]*>', '\\n', html_content)\n",
    "\n",
    "#         # Replace table cells with tab separation\n",
    "#         html_content = re.sub(r'</td>', '\\t', html_content)\n",
    "\n",
    "#         # Remove all HTML tags\n",
    "#         text = re.sub(r'<[^>]+>', ' ', html_content)\n",
    "\n",
    "#         # Decode HTML entities (&nbsp;, &lt;, etc.)\n",
    "#         text = html.unescape(text)\n",
    "\n",
    "#         # Handle literal escape sequences that appear in the text\n",
    "#         # Replace literal \"\\xad\" with empty string (remove soft hyphens)\n",
    "#         text = text.replace('\\\\xad', '')\n",
    "#         # Replace literal \"\\xa0\" with a space (non-breaking spaces)\n",
    "#         text = text.replace('\\\\xa0', ' ')\n",
    "\n",
    "#         # Handle actual Unicode characters too\n",
    "#         # Remove soft hyphens (invisible hyphens used for word breaks)\n",
    "#         text = text.replace('\\xad', '')\n",
    "#         # Replace non-breaking spaces with regular spaces\n",
    "#         text = text.replace('\\xa0', ' ')\n",
    "#         # Remove other problematic control characters\n",
    "#         text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f]', '', text)\n",
    "\n",
    "#         # Clean up other escape sequences that might appear in text\n",
    "#         text = text.replace('\\\\\\\\', '\\\\')  # Double backslash to single\n",
    "#         text = text.replace(\"\\\\'\", \"'\")    # Escaped single quote\n",
    "#         text = text.replace('\\\\\"', '\"')    # Escaped double quote\n",
    "#         text = text.replace('\\\\n', '\\n')   # Literal \\n to newline\n",
    "#         text = text.replace('\\\\t', '\\t')   # Literal \\t to tab\n",
    "\n",
    "#         # Remove remaining literal escape sequences like \\x.. that weren't handled above\n",
    "#         text = re.sub(r'\\\\x[0-9a-fA-F]{2}', '', text)\n",
    "\n",
    "#         # Clean up whitespace (multiple spaces, tabs, newlines)\n",
    "#         text = re.sub(r'[ \\t]+', ' ', text)\n",
    "#         text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "#         # Final cleanup to remove leading/trailing whitespace\n",
    "#         return text.strip()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing HTML: {e}\")\n",
    "#         return f\"Error processing HTML content: {str(e)}\"\n",
    "\n",
    "# def get_email_body(message):\n",
    "#     \"\"\"Extract body text from email message, handling HTML correctly\"\"\"\n",
    "#     body_text = \"\"\n",
    "#     body_html = \"\"\n",
    "\n",
    "#     if message.is_multipart():\n",
    "#         for part in message.walk():\n",
    "#             content_type = part.get_content_type()\n",
    "#             content_disposition = str(part.get(\"Content-Disposition\") or \"\")\n",
    "\n",
    "#             # Skip attachments\n",
    "#             if \"attachment\" in content_disposition:\n",
    "#                 continue\n",
    "\n",
    "#             try:\n",
    "#                 payload = part.get_payload(decode=True)\n",
    "#                 if payload is None:\n",
    "#                     continue\n",
    "\n",
    "#                 charset = part.get_content_charset() or 'utf-8'\n",
    "#                 decoded_payload = payload.decode(charset, errors='replace')\n",
    "\n",
    "#                 if content_type == \"text/plain\":\n",
    "#                     body_text += decoded_payload\n",
    "#                 elif content_type == \"text/html\":\n",
    "#                     body_html += decoded_payload\n",
    "#             except:\n",
    "#                 continue\n",
    "#     else:\n",
    "#         # Not multipart - get payload directly\n",
    "#         try:\n",
    "#             content_type = message.get_content_type()\n",
    "#             payload = message.get_payload(decode=True)\n",
    "#             if payload:\n",
    "#                 charset = message.get_content_charset() or 'utf-8'\n",
    "#                 decoded_payload = payload.decode(charset, errors='replace')\n",
    "\n",
    "#                 if content_type == \"text/plain\":\n",
    "#                     body_text = decoded_payload\n",
    "#                 elif content_type == \"text/html\":\n",
    "#                     body_html = decoded_payload\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#     # Prefer HTML content but fall back to plain text\n",
    "#     if body_html:\n",
    "#         return {\n",
    "#             \"html\": body_html,\n",
    "#             \"text\": extract_clean_text_from_html(body_html),\n",
    "#             \"has_html\": True\n",
    "#         }\n",
    "#     else:\n",
    "#         return {\n",
    "#             \"html\": \"\",\n",
    "#             \"text\": body_text,\n",
    "#             \"has_html\": False\n",
    "#         }\n",
    "\n",
    "# def extract_attachments_info(message):\n",
    "#     \"\"\"Extract information about attachments\"\"\"\n",
    "#     attachments = []\n",
    "\n",
    "#     if message.is_multipart():\n",
    "#         for part in message.walk():\n",
    "#             content_disposition = str(part.get(\"Content-Disposition\") or \"\")\n",
    "\n",
    "#             if \"attachment\" in content_disposition:\n",
    "#                 filename = part.get_filename()\n",
    "#                 if filename:\n",
    "#                     try:\n",
    "#                         filename = decode_str(filename)\n",
    "#                     except:\n",
    "#                         pass\n",
    "\n",
    "#                 content_type = part.get_content_type()\n",
    "#                 content = part.get_payload(decode=True) or b''\n",
    "\n",
    "#                 attachments.append({\n",
    "#                     \"filename\": filename,\n",
    "#                     \"content_type\": content_type,\n",
    "#                     \"size\": len(content),\n",
    "#                     \"content\": content\n",
    "#                 })\n",
    "\n",
    "#     return attachments\n",
    "\n",
    "# def parse_email_address(address_str):\n",
    "#     \"\"\"Parse a string containing email addresses into a list of Entity objects\"\"\"\n",
    "#     if not address_str:\n",
    "#         return []\n",
    "\n",
    "#     entities = []\n",
    "#     # Simple regex to extract name and email from patterns like \"Name <email@example.com>\"\n",
    "#     email_pattern = re.compile(r'(.*?)\\s*<([^>]+)>|([^,\\s]+@[^,\\s]+)')\n",
    "\n",
    "#     for addr in address_str.split(','):\n",
    "#         addr = addr.strip()\n",
    "#         if not addr:\n",
    "#             continue\n",
    "\n",
    "#         match = email_pattern.search(addr)\n",
    "#         if match:\n",
    "#             if match.group(2):  # Format: \"Name <email@example.com>\"\n",
    "#                 name = match.group(1).strip()\n",
    "#                 email_addr = match.group(2).strip()\n",
    "#             else:  # Format: \"email@example.com\"\n",
    "#                 name = email_addr = match.group(3).strip()\n",
    "\n",
    "#             # Create Entity with EmailAddress\n",
    "#             email_obj = EmailAddress(email=email_addr)\n",
    "#             entity = Entity(\n",
    "#                 name=name,\n",
    "#                 email=email_obj,\n",
    "#                 is_physical_person=True  # Assuming default\n",
    "#             )\n",
    "#             entities.append(entity)\n",
    "\n",
    "#     return entities\n",
    "\n",
    "# def extract_recipients(message):\n",
    "#     \"\"\"Extract all recipients (To, CC, BCC) as Entity objects\"\"\"\n",
    "#     to_str = decode_str(message.get('to') or \"\")\n",
    "#     cc_str = decode_str(message.get('cc') or \"\")\n",
    "#     bcc_str = decode_str(message.get('bcc') or \"\")\n",
    "#     reply_to_str = decode_str(message.get('reply-to') or \"\")\n",
    "\n",
    "#     to_entities = parse_email_address(to_str)\n",
    "#     cc_entities = parse_email_address(cc_str)\n",
    "#     bcc_entities = parse_email_address(bcc_str)\n",
    "#     reply_to_entity = parse_email_address(reply_to_str)[0] if parse_email_address(reply_to_str) else None\n",
    "\n",
    "#     return {\n",
    "#         \"to\": to_entities,\n",
    "#         \"cc\": cc_entities,\n",
    "#         \"bcc\": bcc_entities,\n",
    "#         \"reply_to\": reply_to_entity\n",
    "#     }\n",
    "\n",
    "# list_email_match = re.search(r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)', list_unsubscribe)\n",
    "#         if list_email_match:\n",
    "#             list_email = list_email_match.group(1)\n",
    "\n",
    "#         mailing_list = MailingList(\n",
    "#             id=str(uuid.uuid4()),\n",
    "#             name=list_name,\n",
    "#             description=f\"Mailing list extracted from {list_id}\",\n",
    "#             email_address=EmailAddress(email=list_email)\n",
    "#         )\n",
    "\n",
    "#     # Create a SenderEmail object\n",
    "#     sender_email_id = str(uuid.uuid4())\n",
    "#     sender_email = SenderEmail(\n",
    "#         id=sender_email_id,\n",
    "#         sender=sender_entity,\n",
    "#         body=body_content[\"text\"],\n",
    "#         timestamp=timestamp\n",
    "#     )\n",
    "\n",
    "#     # Create a ReceiverEmail object\n",
    "#     receiver_email = ReceiverEmail(\n",
    "#         id=email_id,\n",
    "#         sender_email=sender_email,\n",
    "#         sender=sender_entity,\n",
    "#         to=recipients[\"to\"] if recipients[\"to\"] else None,\n",
    "#         reply_to=recipients[\"reply_to\"],\n",
    "#         cc=recipients[\"cc\"] if recipients[\"cc\"] else None,\n",
    "#         bcc=recipients[\"bcc\"] if recipients[\"bcc\"] else None,\n",
    "#         timestamp=timestamp,\n",
    "#         subject=subject,\n",
    "#         body=body_content[\"text\"],\n",
    "#         attachments=attachments if attachments else None,\n",
    "#         is_deleted=False,\n",
    "#         folder=folder_name,\n",
    "#         is_spam=False,\n",
    "#         mailing_list=mailing_list,\n",
    "#         importance_score=0,  # Default value\n",
    "#         mother_email=None,  # Will be linked later based on in_reply_to\n",
    "#         children_emails=None\n",
    "#     )\n",
    "\n",
    "#     # Create a data dictionary for our normalized database tables\n",
    "#     result = {\n",
    "#         'id': email_id,\n",
    "#         'sender_email_id': sender_email.id,\n",
    "#         'sender_id': None,  # Will be filled in by the processing function\n",
    "#         'reply_to_id': None,  # Will be filled in by the processing function\n",
    "#         'timestamp': timestamp,\n",
    "#         'subject': subject,\n",
    "#         'body': body_content[\"text\"],\n",
    "#         'body_html': body_content[\"html\"] if body_content[\"has_html\"] else None,\n",
    "#         'has_html': body_content[\"has_html\"],\n",
    "#         'is_deleted': False,\n",
    "#         'folder': folder_name,\n",
    "#         'is_spam': False,\n",
    "#         'mailing_list_id': mailing_list.id if mailing_list else None,\n",
    "#         'importance_score': 0,\n",
    "#         'mother_email_id': None,  # Will be updated later based on in_reply_to\n",
    "#         'message_id': message_id,\n",
    "#         'references': references,\n",
    "#         'in_reply_to': in_reply_to\n",
    "#     }\n",
    "\n",
    "#     # Add additional data for cross-reference during insertion\n",
    "#     result['attachments_data'] = attachments_data\n",
    "\n",
    "#     return result, receiver_email\n",
    "\n",
    "# def setup_database(db_path):\n",
    "#     \"\"\"Set up the DuckDB database schema with proper types and indexes\"\"\"\n",
    "#     # Connect to DuckDB database\n",
    "#     conn = duckdb.connect(db_path)\n",
    "\n",
    "#     # Create tables for each Pydantic model\n",
    "\n",
    "#     # Organizations table\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS organizations (\n",
    "#         id VARCHAR PRIMARY KEY,\n",
    "#         name VARCHAR,\n",
    "#         description VARCHAR,\n",
    "#         email_address VARCHAR\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Positions table\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS positions (\n",
    "#         id VARCHAR PRIMARY KEY,\n",
    "#         name VARCHAR,\n",
    "#         start_date TIMESTAMP,\n",
    "#         end_date TIMESTAMP,\n",
    "#         description VARCHAR,\n",
    "#         organization_id VARCHAR,\n",
    "#         FOREIGN KEY (organization_id) REFERENCES organizations(id)\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Entities table (for senders and recipients)\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS entities (\n",
    "#         id VARCHAR PRIMARY KEY,\n",
    "#         name VARCHAR,\n",
    "#         email VARCHAR,\n",
    "#         alias_names JSON,\n",
    "#         is_physical_person BOOLEAN\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Entity alias emails table\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS entity_alias_emails (\n",
    "#         id VARCHAR PRIMARY KEY,\n",
    "#         entity_id VARCHAR,\n",
    "#         email VARCHAR,\n",
    "#         FOREIGN KEY (entity_id) REFERENCES entities(id)\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Entity positions table\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS entity_positions (\n",
    "#         entity_id VARCHAR,\n",
    "#         position_id VARCHAR,\n",
    "#         PRIMARY KEY (entity_id, position_id),\n",
    "#         FOREIGN KEY (entity_id) REFERENCES entities(id),\n",
    "#         FOREIGN KEY (position_id) REFERENCES positions(id)\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Mailing lists table\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS mailing_lists (\n",
    "#         id VARCHAR PRIMARY KEY,\n",
    "#         name VARCHAR,\n",
    "#         description VARCHAR,\n",
    "#         email_address VARCHAR\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Sender emails table\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS sender_emails (\n",
    "#         id VARCHAR PRIMARY KEY,\n",
    "#         sender_id VARCHAR,\n",
    "#         body TEXT,\n",
    "#         timestamp TIMESTAMP,\n",
    "#         FOREIGN KEY (sender_id) REFERENCES entities(id)\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Receiver emails table\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS receiver_emails (\n",
    "#         id VARCHAR PRIMARY KEY,\n",
    "#         sender_email_id VARCHAR,\n",
    "#         sender_id VARCHAR,\n",
    "#         reply_to_id VARCHAR,\n",
    "#         timestamp TIMESTAMP,\n",
    "#         subject VARCHAR,\n",
    "#         body TEXT,\n",
    "#         body_html TEXT,\n",
    "#         has_html BOOLEAN,\n",
    "#         is_deleted BOOLEAN DEFAULT FALSE,\n",
    "#         folder VARCHAR DEFAULT 'inbox',\n",
    "#         is_spam BOOLEAN DEFAULT FALSE,\n",
    "#         mailing_list_id VARCHAR,\n",
    "#         importance_score INTEGER DEFAULT 0,\n",
    "#         mother_email_id VARCHAR,\n",
    "#         message_id VARCHAR,\n",
    "#         references TEXT,\n",
    "#         FOREIGN KEY (sender_email_id) REFERENCES sender_emails(id),\n",
    "#         FOREIGN KEY (sender_id) REFERENCES entities(id),\n",
    "#         FOREIGN KEY (reply_to_id) REFERENCES entities(id),\n",
    "#         FOREIGN KEY (mailing_list_id) REFERENCES mailing_lists(id),\n",
    "#         FOREIGN KEY (mother_email_id) REFERENCES receiver_emails(id)\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Email recipients tables (to, cc, bcc)\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS email_recipients_to (\n",
    "#         email_id VARCHAR,\n",
    "#         entity_id VARCHAR,\n",
    "#         PRIMARY KEY (email_id, entity_id),\n",
    "#         FOREIGN KEY (email_id) REFERENCES receiver_emails(id),\n",
    "#         FOREIGN KEY (entity_id) REFERENCES entities(id)\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS email_recipients_cc (\n",
    "#         email_id VARCHAR,\n",
    "#         entity_id VARCHAR,\n",
    "#         PRIMARY KEY (email_id, entity_id),\n",
    "#         FOREIGN KEY (email_id) REFERENCES receiver_emails(id),\n",
    "#         FOREIGN KEY (entity_id) REFERENCES entities(id)\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS email_recipients_bcc (\n",
    "#         email_id VARCHAR,\n",
    "#         entity_id VARCHAR,\n",
    "#         PRIMARY KEY (email_id, entity_id),\n",
    "#         FOREIGN KEY (email_id) REFERENCES receiver_emails(id),\n",
    "#         FOREIGN KEY (entity_id) REFERENCES entities(id)\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Attachments table\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS attachments (\n",
    "#         id VARCHAR PRIMARY KEY,\n",
    "#         email_id VARCHAR,\n",
    "#         filename VARCHAR,\n",
    "#         content BLOB,\n",
    "#         content_type VARCHAR,\n",
    "#         size INTEGER,\n",
    "#         FOREIGN KEY (email_id) REFERENCES receiver_emails(id)\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Create child email relationships table\n",
    "#     conn.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS email_children (\n",
    "#         parent_id VARCHAR,\n",
    "#         child_id VARCHAR,\n",
    "#         PRIMARY KEY (parent_id, child_id),\n",
    "#         FOREIGN KEY (parent_id) REFERENCES receiver_emails(id),\n",
    "#         FOREIGN KEY (child_id) REFERENCES receiver_emails(id)\n",
    "#     )\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Create indexes\n",
    "#     conn.execute('CREATE INDEX IF NOT EXISTS idx_receiver_emails_timestamp ON receiver_emails(timestamp)')\n",
    "#     conn.execute('CREATE INDEX IF NOT EXISTS idx_receiver_emails_folder ON receiver_emails(folder)')\n",
    "#     conn.execute('CREATE INDEX IF NOT EXISTS idx_receiver_emails_subject ON receiver_emails(subject)')\n",
    "#     conn.execute('CREATE INDEX IF NOT EXISTS idx_receiver_emails_message_id ON receiver_emails(message_id)')\n",
    "#     conn.execute('CREATE INDEX IF NOT EXISTS idx_entities_email ON entities(email)')\n",
    "#     conn.execute('CREATE INDEX IF NOT EXISTS idx_attachments_email_id ON attachments(email_id)')\n",
    "\n",
    "#     return conn\n",
    "\n",
    "# def process_mbox_to_duckdb(mbox_path, conn, batch_size=100, entity_cache=None):\n",
    "#     \"\"\"Process mbox file directly to DuckDB in batches with progress bar\"\"\"\n",
    "#     if entity_cache is None:\n",
    "#         entity_cache = {}  # Cache to store entities we've already seen\n",
    "\n",
    "#     mbox = mailbox.mbox(mbox_path)\n",
    "#     folder_name = os.path.basename(mbox_path).replace('.mbox', '')\n",
    "\n",
    "#     # Count messages for progress bar\n",
    "#     total_messages = len(mbox)\n",
    "#     print(f\"Processing {folder_name} ({total_messages} messages)\")\n",
    "\n",
    "#     # Process in batches for each table\n",
    "#     entity_batch = []\n",
    "#     entity_alias_emails_batch = []\n",
    "#     mailing_list_batch = []\n",
    "#     sender_email_batch = []\n",
    "#     receiver_email_batch = []\n",
    "#     to_recipients_batch = []\n",
    "#     cc_recipients_batch = []\n",
    "#     bcc_recipients_batch = []\n",
    "#     attachments_batch = []\n",
    "\n",
    "#     for i, message in enumerate(tqdm(mbox, total=total_messages, desc=folder_name)):\n",
    "#         email_data, receiver_email = extract_message_data(message, folder_name)\n",
    "\n",
    "#         # Process sender entity\n",
    "#         sender = receiver_email.sender\n",
    "#         if sender.email.email not in entity_cache:\n",
    "#             entity_id = str(uuid.uuid4())\n",
    "#             entity_cache[sender.email.email] = entity_id\n",
    "\n",
    "#             # Add to entities batch\n",
    "#             entity_batch.append({\n",
    "#                 'id': entity_id,\n",
    "#                 'name': sender.name,\n",
    "#                 'email': sender.email.email,\n",
    "#                 'alias_names': json.dumps(sender.alias_names) if sender.alias_names else None,\n",
    "#                 'is_physical_person': sender.is_physical_person\n",
    "#             })\n",
    "\n",
    "#             # Process alias emails if any\n",
    "#             if sender.alias_emails:\n",
    "#                 for alias_email in sender.alias_emails:\n",
    "#                     entity_alias_emails_batch.append({\n",
    "#                         'id': str(uuid.uuid4()),\n",
    "#                         'entity_id': entity_id,\n",
    "#                         'email': alias_email.email\n",
    "#                     })\n",
    "#         else:\n",
    "#             # Get cached entity ID\n",
    "#             entity_id = entity_cache[sender.email.email]\n",
    "\n",
    "#         # Process sender email\n",
    "#         sender_email = receiver_email.sender_email\n",
    "#         sender_email_batch.append({\n",
    "#             'id': sender_email.id,\n",
    "#             'sender_id': entity_id,  # Use cached or new entity ID\n",
    "#             'body': sender_email.body,\n",
    "#             'timestamp': sender_email.timestamp\n",
    "#         })\n",
    "\n",
    "#         # Process receiver email\n",
    "#         reply_to_id = None\n",
    "#         if receiver_email.reply_to:\n",
    "#             reply_to_email = receiver_email.reply_to.email.email\n",
    "#             if reply_to_email not in entity_cache:\n",
    "#                 reply_to_id = str(uuid.uuid4())\n",
    "#                 entity_cache[reply_to_email] = reply_to_id\n",
    "\n",
    "#                 entity_batch.append({\n",
    "#                     'id': reply_to_id,\n",
    "#                     'name': receiver_email.reply_to.name,\n",
    "#                     'email': reply_to_email,\n",
    "#                     'alias_names': None,\n",
    "#                     'is_physical_person': True\n",
    "#                 })\n",
    "#             else:\n",
    "#                 reply_to_id = entity_cache[reply_to_email]\n",
    "\n",
    "#         # Add mailing list if present\n",
    "#         mailing_list_id = None\n",
    "#         if receiver_email.mailing_list:\n",
    "#             mailing_list_id = receiver_email.mailing_list.id\n",
    "#             mailing_list_batch.append({\n",
    "#                 'id': mailing_list_id,\n",
    "#                 'name': receiver_email.mailing_list.name,\n",
    "#                 'description': receiver_email.mailing_list.description,\n",
    "#                 'email_address': receiver_email.mailing_list.email_address.email\n",
    "#             })\n",
    "\n",
    "#         # Add receiver email\n",
    "#         receiver_email_batch.append({\n",
    "#             'id': receiver_email.id,\n",
    "#             'sender_email_id': sender_email.id,\n",
    "#             'sender_id': entity_id,\n",
    "#             'reply_to_id': reply_to_id,\n",
    "#             'timestamp': receiver_email.timestamp,\n",
    "#             'subject': receiver_email.subject,\n",
    "#             'body': receiver_email.body,\n",
    "#             'body_html': email_data['body_html'] if 'body_html' in email_data else None,\n",
    "#             'has_html': email_data['has_html'] if 'has_html' in email_data else False,\n",
    "#             'is_deleted': receiver_email.is_deleted,\n",
    "#             'folder': receiver_email.folder,\n",
    "#             'is_spam': receiver_email.is_spam,\n",
    "#             'mailing_list_id': mailing_list_id,\n",
    "#             'importance_score': receiver_email.importance_score,\n",
    "#             'mother_email_id': None,  # Will be updated later\n",
    "#             'message_id': email_data['message_id'],\n",
    "#             'references': email_data['references'] if 'references' in email_data else None\n",
    "#         })\n",
    "\n",
    "#         # Process recipients (to, cc, bcc)\n",
    "#         if receiver_email.to:\n",
    "#             for entity in receiver_email.to:\n",
    "#                 if entity.email.email not in entity_cache:\n",
    "#                     to_entity_id = str(uuid.uuid4())\n",
    "#                     entity_cache[entity.email.email] = to_entity_id\n",
    "\n",
    "#                     entity_batch.append({\n",
    "#                         'id': to_entity_id,\n",
    "#                         'name': entity.name,\n",
    "#                         'email': entity.email.email,\n",
    "#                         'alias_names': json.dumps(entity.alias_names) if entity.alias_names else None,\n",
    "#                         'is_physical_person': entity.is_physical_person\n",
    "#                     })\n",
    "\n",
    "#                     # Process alias emails\n",
    "#                     if entity.alias_emails:\n",
    "#                         for alias_email in entity.alias_emails:\n",
    "#                             entity_alias_emails_batch.append({\n",
    "#                                 'id': str(uuid.uuid4()),\n",
    "#                                 'entity_id': to_entity_id,\n",
    "#                                 'email': alias_email.email\n",
    "#                             })\n",
    "#                 else:\n",
    "#                     to_entity_id = entity_cache[entity.email.email]\n",
    "\n",
    "#                 # Add to recipients relationship\n",
    "#                 to_recipients_batch.append({\n",
    "#                     'email_id': receiver_email.id,\n",
    "#                     'entity_id': to_entity_id\n",
    "#                 })\n",
    "\n",
    "#         # Process CC recipients\n",
    "#         if receiver_email.cc:\n",
    "#             for entity in receiver_email.cc:\n",
    "#                 if entity.email.email not in entity_cache:\n",
    "#                     cc_entity_id = str(uuid.uuid4())\n",
    "#                     entity_cache[entity.email.email] = cc_entity_id\n",
    "\n",
    "#                     entity_batch.append({\n",
    "#                         'id': cc_entity_id,\n",
    "#                         'name': entity.name,\n",
    "#                         'email': entity.email.email,\n",
    "#                         'alias_names': json.dumps(entity.alias_names) if entity.alias_names else None,\n",
    "#                         'is_physical_person': entity.is_physical_person\n",
    "#                     })\n",
    "#                 else:\n",
    "#                     cc_entity_id = entity_cache[entity.email.email]\n",
    "\n",
    "#                 # Add cc recipients relationship\n",
    "#                 cc_recipients_batch.append({\n",
    "#                     'email_id': receiver_email.id,\n",
    "#                     'entity_id': cc_entity_id\n",
    "#                 })\n",
    "\n",
    "#         # Process BCC recipients\n",
    "#         if receiver_email.bcc:\n",
    "#             for entity in receiver_email.bcc:\n",
    "#                 if entity.email.email not in entity_cache:\n",
    "#                     bcc_entity_id = str(uuid.uuid4())\n",
    "#                     entity_cache[entity.email.email] = bcc_entity_id\n",
    "\n",
    "#                     entity_batch.append({\n",
    "#                         'id': bcc_entity_id,\n",
    "#                         'name': entity.name,\n",
    "#                         'email': entity.email.email,\n",
    "#                         'alias_names': json.dumps(entity.alias_names) if entity.alias_names else None,\n",
    "#                         'is_physical_person': entity.is_physical_person\n",
    "#                     })\n",
    "#                 else:\n",
    "#                     bcc_entity_id = entity_cache[entity.email.email]\n",
    "\n",
    "#                 # Add bcc recipients relationship\n",
    "#                 bcc_recipients_batch.append({\n",
    "#                     'email_id': receiver_email.id,\n",
    "#                     'entity_id': bcc_entity_id\n",
    "#                 })\n",
    "\n",
    "#         # Process attachments\n",
    "#         if receiver_email.attachments:\n",
    "#             for attachment in receiver_email.attachments:\n",
    "#                 attachments_batch.append({\n",
    "#                     'id': str(uuid.uuid4()),\n",
    "#                     'email_id': receiver_email.id,\n",
    "#                     'filename': attachment.filename,\n",
    "#                     'content': attachment.content,\n",
    "#                     'content_type': email_data.get('attachments_json', {}).get('content_type', 'application/octet-stream'),\n",
    "#                     'size': len(attachment.content) if attachment.content else 0\n",
    "#                 })\n",
    "\n",
    "#         # Process batch when it reaches the batch size or on the last message\n",
    "#         if len(receiver_email_batch) >= batch_size or i == total_messages - 1:\n",
    "#             # Insert entities\n",
    "#             if entity_batch:\n",
    "#                 entities_df = pd.DataFrame(entity_batch)\n",
    "#                 conn.execute(\"\"\"\n",
    "#                 INSERT OR IGNORE INTO entities\n",
    "#                 SELECT * FROM entities_df\n",
    "#                 \"\"\")\n",
    "#                 entity_batch = []\n",
    "\n",
    "#             # Insert entity alias emails\n",
    "#             if entity_alias_emails_batch:\n",
    "#                 alias_emails_df = pd.DataFrame(entity_alias_emails_batch)\n",
    "#                 conn.execute(\"\"\"\n",
    "#                 INSERT OR IGNORE INTO entity_alias_emails\n",
    "#                 SELECT * FROM alias_emails_df\n",
    "#                 \"\"\")\n",
    "#                 entity_alias_emails_batch = []\n",
    "\n",
    "#             # Insert mailing lists\n",
    "#             if mailing_list_batch:\n",
    "#                 mailing_lists_df = pd.DataFrame(mailing_list_batch)\n",
    "#                 conn.execute(\"\"\"\n",
    "#                 INSERT OR IGNORE INTO mailing_lists\n",
    "#                 SELECT * FROM mailing_lists_df\n",
    "#                 \"\"\")\n",
    "#                 mailing_list_batch = []\n",
    "\n",
    "#             # Insert sender emails\n",
    "#             if sender_email_batch:\n",
    "#                 sender_emails_df = pd.DataFrame(sender_email_batch)\n",
    "#                 conn.execute(\"\"\"\n",
    "#                 INSERT OR IGNORE INTO sender_emails\n",
    "#                 SELECT * FROM sender_emails_df\n",
    "#                 \"\"\")\n",
    "#                 sender_email_batch = []\n",
    "\n",
    "#             # Insert receiver emails\n",
    "#             if receiver_email_batch:\n",
    "#                 receiver_emails_df = pd.DataFrame(receiver_email_batch)\n",
    "#                 conn.execute(\"\"\"\n",
    "#                 INSERT OR IGNORE INTO receiver_emails\n",
    "#                 SELECT * FROM receiver_emails_df\n",
    "#                 \"\"\")\n",
    "#                 receiver_email_batch = []\n",
    "\n",
    "#             # Insert recipient relationships\n",
    "#             if to_recipients_batch:\n",
    "#                 to_recipients_df = pd.DataFrame(to_recipients_batch)\n",
    "#                 conn.execute(\"\"\"\n",
    "#                 INSERT OR IGNORE INTO email_recipients_to\n",
    "#                 SELECT * FROM to_recipients_df\n",
    "#                 \"\"\")\n",
    "#                 to_recipients_batch = []\n",
    "\n",
    "#             if cc_recipients_batch:\n",
    "#                 cc_recipients_df = pd.DataFrame(cc_recipients_batch)\n",
    "#                 conn.execute(\"\"\"\n",
    "#                 INSERT OR IGNORE INTO email_recipients_cc\n",
    "#                 SELECT * FROM cc_recipients_df\n",
    "#                 \"\"\")\n",
    "#                 cc_recipients_batch = []\n",
    "\n",
    "#             if bcc_recipients_batch:\n",
    "#                 bcc_recipients_df = pd.DataFrame(bcc_recipients_batch)\n",
    "#                 conn.execute(\"\"\"\n",
    "#                 INSERT OR IGNORE INTO email_recipients_bcc\n",
    "#                 SELECT * FROM bcc_recipients_df\n",
    "#                 \"\"\")\n",
    "#                 bcc_recipients_batch = []\n",
    "\n",
    "#             # Insert attachments\n",
    "#             if attachments_batch:\n",
    "#                 attachments_df = pd.DataFrame(attachments_batch)\n",
    "#                 conn.execute(\"\"\"\n",
    "#                 INSERT OR IGNORE INTO attachments\n",
    "#                 SELECT * FROM attachments_df\n",
    "#                 \"\"\")\n",
    "#                 attachments_batch = []\n",
    "\n",
    "#             # Commit to save progress\n",
    "#             conn.commit()\n",
    "\n",
    "#     return entity_cache\n",
    "\n",
    "# def process_mbox_files(directory, output_path=None):\n",
    "#     \"\"\"\n",
    "#     Process mbox files and save to DuckDB format with normalized tables\n",
    "\n",
    "#     Parameters:\n",
    "#     - directory: Directory containing .mbox files\n",
    "#     - output_path: Output file path (default: emails.duckdb)\n",
    "#     \"\"\"\n",
    "#     # Set default output path if not provided\n",
    "#     if output_path is None:\n",
    "#         output_path = 'emails.duckdb'\n",
    "#     elif not output_path.endswith('.duckdb'):\n",
    "#         output_path = f\"{output_path}.duckdb\"\n",
    "\n",
    "#     # Setup database\n",
    "#     conn = setup_database(output_path)\n",
    "\n",
    "#     # Get list of mbox files\n",
    "#     mbox_files = [f for f in os.listdir(directory) if f.endswith('.mbox')]\n",
    "#     print(f\"Found {len(mbox_files)} mbox files\")\n",
    "\n",
    "#     # Entity cache to avoid duplicates across files\n",
    "#     entity_cache = {}\n",
    "\n",
    "#     # Process each file\n",
    "#     for filename in mbox_files:\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         entity_cache = process_mbox_to_duckdb(file_path, conn, entity_cache=entity_cache)\n",
    "\n",
    "#     # Create relationships between emails (mother/child relationships)\n",
    "#     print(\"Creating email thread relationships...\")\n",
    "#     conn.execute(\"\"\"\n",
    "#     UPDATE receiver_emails\n",
    "#     SET mother_email_id = (\n",
    "#         SELECT r2.id\n",
    "#         FROM receiver_emails r2\n",
    "#         WHERE r2.message_id = receiver_emails.in_reply_to\n",
    "#         LIMIT 1\n",
    "#     )\n",
    "#     WHERE in_reply_to IS NOT NULL\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Populate the children relationships table\n",
    "#     print(\"Populating child email relationships...\")\n",
    "#     conn.execute(\"\"\"\n",
    "#     INSERT INTO email_children (parent_id, child_id)\n",
    "#     SELECT mother_email_id, id\n",
    "#     FROM receiver_emails\n",
    "#     WHERE mother_email_id IS NOT NULL\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Final optimization and cleanup\n",
    "#     print(\"Optimizing database...\")\n",
    "#     conn.execute(\"PRAGMA optimize\")\n",
    "#     conn.close()\n",
    "\n",
    "#     print(f\"DuckDB database saved to {output_path}\")\n",
    "#     print(\"\"\"\n",
    "# Database structure:\n",
    "# - entities: Stores all senders and recipients\n",
    "# - entity_alias_emails: Stores alias emails for entities\n",
    "# - sender_emails: Stores email data from senders\n",
    "# - receiver_emails: Stores received email data\n",
    "# - email_recipients_to/cc/bcc: Links emails to recipient entities\n",
    "# - attachments: Stores email attachments\n",
    "# - email_children: Stores parent-child relationships between emails\n",
    "# - mailing_lists: Stores mailing list information\n",
    "# - organizations: Stores organization information\n",
    "# - positions: Stores position information\n",
    "# - entity_positions: Links entities to positions\n",
    "# \"\"\")\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser(description='Process mbox files into DuckDB with Pydantic models')\n",
    "#     parser.add_argument('directory', help='Directory containing .mbox files')\n",
    "#     parser.add_argument('--output', help='Output file path (without extension)')\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     process_mbox_files(\n",
    "#         args.directory,\n",
    "#         args.output\n",
    "#     )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6962dfcd",
   "metadata": {},
   "source": [
    "## Eml Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810dc472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aspose.email.storage.mbox import MboxStorageReader, MboxLoadOptions\n",
    "\n",
    "# mbox_load_options = MboxLoadOptions()\n",
    "# mbox_load_options.leave_open = False\n",
    "# mbox_load_options.preferred_text_encoding = 'utf-8'\n",
    "\n",
    "# with MboxStorageReader.create_reader(\"data/processed/mailbox_cecile/AG.mbox\", mbox_load_options) as mbox_reader:\n",
    "#     for eml in mbox_reader.enumerate_messages():\n",
    "#         eml.save(f\"{eml.subject}.eml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f35b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 6 email files to data/processed/mailbox_cecile_mbox_to_eml\n",
      "Number of .eml files in directory: 6\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# from aspose.email.storage.mbox import MboxStorageReader, MboxLoadOptions\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Define the input and output directories\n",
    "# input_dir = \"data/processed/mailbox_cecile\"\n",
    "# output_dir = \"data/processed/mailbox_cecile_mbox_to_eml\"\n",
    "\n",
    "# # Create the output directory if it doesn't exist\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Configure MBox loading options\n",
    "# mbox_load_options = MboxLoadOptions()\n",
    "# mbox_load_options.leave_open = False\n",
    "# mbox_load_options.preferred_text_encoding = 'utf-8'\n",
    "\n",
    "# # Counter for saved files\n",
    "# total_saved_count = 0\n",
    "\n",
    "# # Get all .mbox files in the input directory\n",
    "# mbox_files = glob.glob(os.path.join(input_dir, \"*.mbox\"))\n",
    "\n",
    "# print(f\"Found {len(mbox_files)} .mbox files to process\")\n",
    "\n",
    "# # Process each mbox file\n",
    "# for mbox_file in mbox_files:\n",
    "#     mbox_name = os.path.basename(mbox_file).replace('.mbox', '')\n",
    "#     file_saved_count = 0\n",
    "\n",
    "#     print(f\"Processing {mbox_name}.mbox...\")\n",
    "\n",
    "#     with MboxStorageReader.create_reader(mbox_file, mbox_load_options) as mbox_reader:\n",
    "#         for eml in mbox_reader.enumerate_messages():\n",
    "#             # Create a safe filename from the subject\n",
    "#             # If subject is empty, use a placeholder\n",
    "#             subject = eml.subject if eml.subject else \"no_subject\"\n",
    "#             safe_subject = \"\".join(c if c.isalnum() or c in [' ', '.', '_', '-'] else '_' for c in subject)\n",
    "\n",
    "#             # Add mbox name and index to ensure unique filenames\n",
    "#             filename = f\"{mbox_name}_{file_saved_count+1:04d}_{safe_subject}.eml\"\n",
    "#             output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "#             # Save the email\n",
    "#             eml.save(output_path)\n",
    "#             file_saved_count += 1\n",
    "#             total_saved_count += 1\n",
    "\n",
    "#     print(f\"  - Saved {file_saved_count} emails from {mbox_name}.mbox\")\n",
    "\n",
    "# # Verify the count matches the number of files in the directory\n",
    "# actual_file_count = len([f for f in os.listdir(output_dir) if f.endswith('.eml')])\n",
    "\n",
    "# # Print summary\n",
    "# print(f\"\\nProcessing complete!\")\n",
    "# print(f\"Total emails saved: {total_saved_count}\")\n",
    "# print(f\"Number of .eml files in directory: {actual_file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbdf8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 .mbox files to process\n",
      "Processing AG.mbox...\n",
      "  - Saved 6 emails from AG.mbox\n",
      "Processing AG.mbox...\n",
      "  - Saved 0 emails from AG.mbox\n",
      "Processing Archive.mbox...\n",
      "  - Saved 10 emails from Archive.mbox\n",
      "Processing Archives calssifiees.mbox...\n",
      "  - Saved 50 emails from Archives calssifiees.mbox\n",
      "Processing Ateliers.mbox...\n",
      "  - Saved 28 emails from Ateliers.mbox\n",
      "Processing Boîte de réception.mbox...\n",
      "  - Saved 50 emails from Boîte de réception.mbox\n",
      "Processing Brouillons.mbox...\n",
      "  - Saved 41 emails from Brouillons.mbox\n",
      "Processing Conflit.mbox...\n",
      "  - Saved 6 emails from Conflit.mbox\n",
      "Processing Courrier indésirable.mbox...\n",
      "  - Saved 45 emails from Courrier indésirable.mbox\n",
      "Processing Formation à distance.mbox...\n",
      "  - Saved 2 emails from Formation à distance.mbox\n",
      "Processing Gazette.mbox...\n",
      "  - Saved 10 emails from Gazette.mbox\n",
      "Processing gestioncrise.mbox...\n",
      "  - Saved 50 emails from gestioncrise.mbox\n",
      "Processing Idees.mbox...\n",
      "  - Saved 18 emails from Idees.mbox\n",
      "Processing Instances.mbox...\n",
      "  - Saved 50 emails from Instances.mbox\n",
      "Processing Plaidoyer.mbox...\n",
      "  - Saved 38 emails from Plaidoyer.mbox\n",
      "Processing RH.mbox...\n",
      "  - Saved 40 emails from RH.mbox\n",
      "Processing Éléments envoyés.mbox...\n",
      "  - Saved 50 emails from Éléments envoyés.mbox\n",
      "Processing Éléments supprimés.mbox...\n",
      "  - Saved 50 emails from Éléments supprimés.mbox\n",
      "\n",
      "Processing complete!\n",
      "Total emails saved: 544\n",
      "Number of .eml files in directory: 544\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from aspose.email.storage.mbox import MboxStorageReader, MboxLoadOptions\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the input and output directories\n",
    "input_dir = \"data/processed/mailbox_cecile\"\n",
    "output_dir = \"data/processed/mailbox_cecile_mbox_to_eml\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Configure MBox loading options\n",
    "mbox_load_options = MboxLoadOptions()\n",
    "mbox_load_options.leave_open = False\n",
    "mbox_load_options.preferred_text_encoding = 'utf-8'\n",
    "\n",
    "# Counter for saved files\n",
    "total_saved_count = 0\n",
    "\n",
    "# Get all .mbox files in the input directory\n",
    "mbox_files = glob.glob(os.path.join(input_dir, \"*.mbox\"))\n",
    "\n",
    "print(f\"Found {len(mbox_files)} .mbox files to process\")\n",
    "\n",
    "# Function to clean folder/file names for database compatibility\n",
    "def clean_name(name):\n",
    "    # Replace spaces and special characters with underscores\n",
    "    # Keep only alphanumeric characters, underscores, and dots\n",
    "    cleaned = re.sub(r'[^\\w\\.]', '_', name)\n",
    "    # Convert to lowercase for consistency\n",
    "    return cleaned.lower()\n",
    "\n",
    "# Process each mbox file\n",
    "for mbox_file in mbox_files:\n",
    "    mbox_name = os.path.basename(mbox_file).replace('.mbox', '')\n",
    "    # cleaned_mbox_name = clean_name(mbox_name)\n",
    "    file_saved_count = 0\n",
    "\n",
    "    print(f\"Processing {mbox_name}.mbox...\")\n",
    "\n",
    "    with MboxStorageReader.create_reader(mbox_file, mbox_load_options) as mbox_reader:\n",
    "        for eml in mbox_reader.enumerate_messages():\n",
    "            # Create a safe filename from the subject\n",
    "            # If subject is empty, use a placeholder\n",
    "            subject = eml.subject if eml.subject else \"no_subject\"\n",
    "            safe_subject = clean_name(subject)\n",
    "\n",
    "            # Format: 000000001_cleaned_folder_name_cleaned_subject.eml\n",
    "            filename = f\"{file_saved_count+1:10d}_{mbox_name}_{safe_subject}.eml\"\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # Save the email\n",
    "            eml.save(output_path)\n",
    "            file_saved_count += 1\n",
    "            total_saved_count += 1\n",
    "\n",
    "    print(f\"  - Saved {file_saved_count} emails from {mbox_name}.mbox\")\n",
    "\n",
    "# Verify the count matches the number of files in the directory\n",
    "actual_file_count = len([f for f in os.listdir(output_dir) if f.endswith('.eml')])\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Total emails saved: {total_saved_count}\")\n",
    "print(f\"Number of .eml files in directory: {actual_file_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olkoa_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
